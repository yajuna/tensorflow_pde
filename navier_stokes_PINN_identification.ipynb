{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz+Z46wReIx8fPYMHQ04Go",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yajuna/tensorflow_pde/blob/master/navier_stokes_PINN_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This work is based on the methodology originally developed by [Raissi](https://github.com/maziarraissi/PINNs/blob/master/main/continuous_time_identification%20(Navier-Stokes)/NavierStokes.py), and the Tensorflow 2.0 modification by [Blechschmidt](https://github.com/janblechschmidt/PDEsByNNs) and [pierremtb ](https://github.com/pierremtb/PINNs-TF2.0).\n",
        "\n",
        "A Tensorflow 2.0 implementation of the inference problem for the Navier Stokes equation is [here](https://github.com/yajuna/tensorflow_pde/blob/master/navier_stokes_PINN_Solver.ipynb). This notebook is a Tensorflow 2.0 implementation of the Navier Stokes identification problem. We run this notebook with the original data provided by the author to test our implementation for the inference problem. This code is modified from Blechschmidt, as well as [the heat identification code](https://github.com/yajuna/tensorflow_pde/blob/master/Heat_equation_with_tensorflow.ipynb)\n",
        "\n",
        "``tf.experimental.numpy.experimental_enable_numpy_behavior()`` helps numpy commands such as ``.min()`` for tf eager tensors."
      ],
      "metadata": {
        "id": "VXtYqqellzJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import scipy.io\n",
        "\n",
        "from time import time\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "fuZGtDwTqSfJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data for Navier Stokes\n",
        "\n",
        "Clone Raissi's original data for Navier Stokes equation."
      ],
      "metadata": {
        "id": "rXjg964rQD_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/maziarraissi/PINNs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8exWF5rNqrPF",
        "outputId": "980e05bf-4426-489d-e9b6-5ac5995a8f05"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PINNs' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define path for data and Python scripts from Raissi."
      ],
      "metadata": {
        "id": "2bJ8HB2rcOAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
        "repoPath = os.path.join(\".\", \"PINNs\")\n",
        "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
        "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
        "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
        "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")"
      ],
      "metadata": {
        "id": "WfPE290ZrG2-"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define data type and set seed for reproducibility involving random choices."
      ],
      "metadata": {
        "id": "gByNK5d9caXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set data type\n",
        "DTYPE='float32'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "tf.random.set_seed(0)"
      ],
      "metadata": {
        "id": "LYUP9e9v5Rug"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define number of training points ``N_train``. Read all data, ``U_star`` (with $u$ and $v$), ``P_star``, ``t_star``, and ``X_star`` (with $x$ and $y$).\n",
        "\n",
        "Data reshaped and stored in ``x,y,t,u,v,p``.\n",
        "\n",
        "A random instance was chosen by ``idx``, then a set of training points are chosen. Testing data is picked from the original data defined above and not randomized.\n",
        "\n",
        "Training data is listed in ``X`` and ``data``, testsing data is listed in ``X_test`` and ``data_test``."
      ],
      "metadata": {
        "id": "hMRvtKWCcoeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_train = 5000\n",
        "\n",
        "path = os.path.join(dataPath, \"cylinder_nektar_wake.mat\")\n",
        "data = scipy.io.loadmat(path)\n",
        "\n",
        "# define training and testing data\n",
        "U_star = data['U_star'] # N x 2 x T\n",
        "P_star = data['p_star'] # N x T\n",
        "t_star = data['t'] # T x 1\n",
        "X_star = data['X_star'] # N x 2\n",
        "\n",
        "N = X_star.shape[0]\n",
        "T = t_star.shape[0]\n",
        "\n",
        "print('N and T', N, T)\n",
        "\n",
        "#Rearrange Data\n",
        "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "TT = np.tile(t_star, (1,N)).T # N x T\n",
        "\n",
        "UU = U_star[:,0,:] # N x T\n",
        "VV = U_star[:,1,:] # N x T\n",
        "PP = P_star # N x T\n",
        "\n",
        "x = XX.flatten()[:,None] # NT x 1\n",
        "y = YY.flatten()[:,None] # NT x 1\n",
        "t = TT.flatten()[:,None] # NT x 1\n",
        "\n",
        "u = UU.flatten()[:,None] # NT x 1\n",
        "v = VV.flatten()[:,None] # NT x 1\n",
        "p = PP.flatten()[:,None] # NT x 1\n",
        "\n",
        "######################################################################\n",
        "######################## Noiseles Data ###############################\n",
        "######################################################################\n",
        "# Training Data\n",
        "idx = np.random.choice(N*T, N_train, replace=False)\n",
        "x_train = x[idx,:]\n",
        "y_train = y[idx,:]\n",
        "t_train = t[idx,:]\n",
        "u_train = u[idx,:]\n",
        "v_train = v[idx,:]\n",
        "p_train = p[idx,:]\n",
        "\n",
        "# Test Data\n",
        "snap = np.array([100])\n",
        "x_star = X_star[:,0:1]\n",
        "y_star = X_star[:,1:2]\n",
        "t_star = TT[:,snap]\n",
        "\n",
        "u_star = U_star[:,0,snap]\n",
        "v_star = U_star[:,1,snap]\n",
        "p_star = P_star[:,snap]\n",
        "\n",
        "# make training and testing data\n",
        "X = tf.concat([x_train,y_train,t_train], axis = 1)\n",
        "data = tf.concat([u_train,v_train,p_train], axis = 1)\n",
        "\n",
        "# print boundary range\n",
        "print(x.min())\n",
        "print(x.max())\n",
        "print(y.min())\n",
        "print(y.max())\n",
        "print(t.min())\n",
        "print(t.max())\n",
        "lb = tf.constant([x.min(), y.min(), t.min()],dtype = DTYPE)\n",
        "ub = tf.constant([x.max(), y.max(), t.max()],dtype = DTYPE)\n",
        "\n",
        "X_test = tf.concat([x_star,y_star,t_star], axis = 1)\n",
        "data_test = tf.concat([u_star,v_star,p_star], axis = 1)\n",
        "\n",
        "print('size of x_trian and x_star', x_train.size, x_star.size)\n",
        "\n",
        "## visualize training data for comparison"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifq23Johr2J-",
        "outputId": "44c1c2cc-54ce-44a1-c4a8-0b7c3a5a6e1f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N and T 5000 200\n",
            "1.0\n",
            "8.0\n",
            "-2.0\n",
            "2.0\n",
            "0.0\n",
            "19.900000000000002\n",
            "size of x_trian and x_star 5000 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print data to compare with OpenFoam generated data."
      ],
      "metadata": {
        "id": "HY7PGAEny763"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print information from Raissi training data\n",
        "\"\"\"\n",
        "Recall training data is defined from\n",
        "\n",
        "U_star = data['U_star'] # N x 2 x T\n",
        "P_star = data['p_star'] # N x T\n",
        "t_star = data['t'] # T x 1\n",
        "X_star = data['X_star'] # N x 2\n",
        "\n",
        "then rearranged to be\n",
        "#Rearrange Data\n",
        "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "TT = np.tile(t_star, (1,N)).T # N x T\n",
        "\n",
        "UU = U_star[:,0,:] # N x T\n",
        "VV = U_star[:,1,:] # N x T\n",
        "PP = P_star # N x T\n",
        "\n",
        "x = XX.flatten()[:,None] # NT x 1\n",
        "y = YY.flatten()[:,None] # NT x 1\n",
        "t = TT.flatten()[:,None] # NT x 1\n",
        "\n",
        "u = UU.flatten()[:,None] # NT x 1\n",
        "v = VV.flatten()[:,None] # NT x 1\n",
        "p = PP.flatten()[:,None] # NT x 1\n",
        "\"\"\"\n",
        "\n",
        "print('max and min of UU', UU.max(), UU.min())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8THi02pozFpp",
        "outputId": "49565437-4140-4544-b6d9-7dda52737457"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max and min of UU 1.322556473239624 -0.24026849014902615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ################### load data from google drive\n",
        "# \"\"\"\n",
        "# To load data from Google Drive, download data from\n",
        "# https://github.com/maziarraissi/PINNs/tree/master/main/Data\n",
        "# and save it in a folder called \"data\" in your Google drive\n",
        "# \"\"\"\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# data = scipy.io.loadmat('/content/drive/My Drive/data/cylinder_nektar_wake.mat')"
      ],
      "metadata": {
        "id": "vB6m2Fx1QIBN"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define PINN_NeuralNet, define model architecture.\n",
        "\n",
        "Input is three dimensional, consisting of $x$, $y$, and $t$. The output is one dimensional, consisting of concatenated $u$, $v$, and $p$.\n",
        "\n",
        "The neural net has one scaling layer, several hidden dense layers, and one output layer."
      ],
      "metadata": {
        "id": "rWgcSz_FlfYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model architecture\n",
        "\n",
        "class PINN_NeuralNet(tf.keras.Model):\n",
        "  \"\"\"Basic architecture of the PINN model\n",
        "  input dimension = 3, for x, y, t\n",
        "  output dimension = 1 for u, v, and p concatenated\n",
        "  \"\"\"\n",
        "# change input from lb, and ub to data from Raissi\n",
        "  def __init__(self, lb, ub,\n",
        "               output_dim = 1,\n",
        "               num_hidden_layers = 8,\n",
        "               num_neurons_per_layer = 20,\n",
        "               activation = 'tanh',\n",
        "               kernel_initializer = 'glorot_normal',\n",
        "               **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    self.lb = lb\n",
        "    self.ub = ub\n",
        "\n",
        "    # X = tf.concat([x,y,t], axis=1)\n",
        "    # self.lb = X.min(0)\n",
        "    # self.ub = X.max(0)\n",
        "    # self.X = X\n",
        "    # self.x = x\n",
        "    # self.y = y\n",
        "    # self.t = t\n",
        "    # self.u = u\n",
        "    # self.v = v\n",
        "\n",
        "    # Define NN architecture\n",
        "    self.scale = tf.keras.layers.Lambda(\n",
        "            lambda x: 2.0*(x - self.lb)/(self.ub - self.lb) - 1.0)\n",
        "    self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "                             activation=tf.keras.activations.get(activation),\n",
        "                             kernel_initializer=kernel_initializer)\n",
        "                           for _ in range(self.num_hidden_layers)]\n",
        "    self.out = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "  def call(self, X):\n",
        "    \"\"\"\n",
        "    Forward-pass thru NN\n",
        "    \"\"\"\n",
        "    Z = self.scale(X)\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      Z = self.hidden[i](Z)\n",
        "    return self.out(Z)"
      ],
      "metadata": {
        "id": "zYzfUI9wtBRW"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define PINNIdentificationNet, inherites from PINN_NeuralNet class.\n",
        "\n",
        "This neural net includes the two parameters to identify, call them ``lambd_1`` and ``lambd_2``."
      ],
      "metadata": {
        "id": "EZ2klcVdx-T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PINNIdentificationNet(PINN_NeuralNet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # Call init of base class\n",
        "        super().__init__(*args,**kwargs)\n",
        "\n",
        "        # Initialize variable for lambda\n",
        "\n",
        "        self.lambd1 = tf.Variable(1.0, trainable=True, dtype=DTYPE)\n",
        "        self.lambd1_list = []\n",
        "\n",
        "        self.lambd2 = tf.Variable(1.0, trainable=True, dtype=DTYPE)\n",
        "        self.lambd2_list = []"
      ],
      "metadata": {
        "id": "MHfKtKPptIAf"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define PINNSolver class.\n",
        "\n",
        "Read in"
      ],
      "metadata": {
        "id": "sctjRp720ikd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PINNSolver():\n",
        "    def __init__(self, model, X_r):\n",
        "        self.model = model\n",
        "\n",
        "        # Store collocation points, separate t and x\n",
        "        self.x = X_r[:,0:1]\n",
        "        self.y = X_r[:,1:2]\n",
        "        self.t = X_r[:,2:3]\n",
        "\n",
        "        # Initialize history of losses and global iteration counter\n",
        "        self.hist = []\n",
        "        self.iter = 0\n",
        "\n",
        "# differentiate to compute residual of PDE, physics information\n",
        "    def get_r(self):\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Watch variables representing t and x during this GradientTape\n",
        "            tape.watch(self.x)\n",
        "            tape.watch(self.y)\n",
        "            tape.watch(self.t)\n",
        "\n",
        "            # compute current psi and p\n",
        "            pp = self.model(tf.stack([self.x[:,0], self.y[:,0],self.t[:,0]], axis=1))\n",
        "            psi = pp[:,0:1]\n",
        "            p = pp[:,1:2]\n",
        "\n",
        "            # Compute gradients\n",
        "            u = tape.gradient(psi, self.y)\n",
        "            v = -tape.gradient(psi, self.x)\n",
        "\n",
        "            u_x = tape.gradient(u, self.x)\n",
        "            u_y = tape.gradient(u, self.y)\n",
        "\n",
        "            v_x = tape.gradient(v, self.x)\n",
        "            v_y = tape.gradient(v, self.y)\n",
        "\n",
        "\n",
        "\n",
        "        p_x = tape.gradient(p, self.x)\n",
        "        p_y = tape.gradient(p, self.y)\n",
        "\n",
        "        u_t = tape.gradient(u, self.t)\n",
        "        u_xx = tape.gradient(u_x, self.x)\n",
        "        u_yy = tape.gradient(u_y, self.y)\n",
        "\n",
        "        v_t = tape.gradient(v, self.t)\n",
        "        v_xx = tape.gradient(v_x, self.x)\n",
        "        v_yy = tape.gradient(v_y, self.y)\n",
        "\n",
        "        del tape\n",
        "\n",
        "        return self.fun_r(u, u_t, u_x, u_y, u_xx, u_yy, v, v_t, v_x, v_y, v_xx, v_yy, p_x, p_y)\n",
        "\n",
        "# compute loss function- physics information + computational error\n",
        "    def loss_fn(self, X, data):\n",
        "\n",
        "        # Compute phi_r from model and X_r\n",
        "        r = self.get_r()\n",
        "        phi_r = tf.reduce_mean(tf.square(r))\n",
        "\n",
        "        # Initialize loss\n",
        "        loss = phi_r\n",
        "\n",
        "        # compute loss at input data- concatenated u, v, p minus computed u, v, p\n",
        "        data_pred = self.model(X)\n",
        "        loss += tf.reduce_mean(tf.square(data - data_pred))\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "# from the model, get model trainable variables and keep for gradient descent\n",
        "    def get_grad(self, X, data):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # This tape is for derivatives with\n",
        "            # respect to trainable variables\n",
        "            tape.watch(self.model.trainable_variables)\n",
        "            loss = self.loss_fn(X, data)\n",
        "\n",
        "        g = tape.gradient(loss, self.model.trainable_variables)\n",
        "        del tape\n",
        "\n",
        "        return loss, g\n",
        "\n",
        "# compute residual from the neural net; physics information\n",
        "    def fun_r(self, u, u_t, u_x, u_y, u_xx, u_yy, v, v_t, v_x, v_y, v_xx, v_yy, p_x, p_y):\n",
        "        \"\"\"Residual of the PDE\"\"\"\n",
        "        f = u_t + self.model.lambd1 * (u * u_x + v * u_y) + p_x - self.model.lambd2 * (u_xx + u_yy)\n",
        "        g = v_t + self.model.lambd1 * (u * v_x + v * v_y) + p_y - self.model.lambd2 * (v_xx + v_yy)\n",
        "        return tf.concat([f, g], axis=1)\n",
        "\n",
        "    def solve_with_TFoptimizer(self, optimizer, X, data, N=1001):\n",
        "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
        "\n",
        "        @tf.function\n",
        "        def train_step():\n",
        "            loss, grad_theta = self.get_grad(X, data)\n",
        "\n",
        "            # Perform gradient descent step\n",
        "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
        "            return loss\n",
        "\n",
        "        for i in range(N):\n",
        "\n",
        "            loss = train_step()\n",
        "\n",
        "            self.current_loss = loss.numpy()\n",
        "            self.callback()\n",
        "\n",
        "    def callback(self, xr=None):\n",
        "        lambd1 = self.model.lambd1.numpy()\n",
        "        self.model.lambd1_list.append(lambd1)\n",
        "\n",
        "        lambd2 = self.model.lambd2.numpy()\n",
        "        self.model.lambd2_list.append(lambd2)\n",
        "\n",
        "        if self.iter % 100 == 0:\n",
        "            print('It {:05d}: loss = {:10.8e} lambda1 = {:10.8e} lambda2 = {:10.8e}'.format(self.iter, self.current_loss, lambd1, lambd2))\n",
        "\n",
        "        self.hist.append(self.current_loss)\n",
        "        self.iter += 1\n",
        "\n",
        "\n",
        "    # def plot_solution(self, **kwargs):\n",
        "    #     N = 411\n",
        "    #     tspace = np.linspace(self.model.lb[0], self.model.ub[0], N)\n",
        "    #     xspace = np.linspace(self.model.lb[1], self.model.ub[1], N)\n",
        "    #     yspace = np.linspace(self.model.lb[2], self.model.ub[2], N)\n",
        "    #     T, X, Y = np.meshgrid(tspace, xspace, yspace)\n",
        "    #     Xgrid = np.vstack([T.flatten(),X.flatten(),Y.flatten()]).T\n",
        "    #     upred = self.model(tf.cast(Xgrid,DTYPE))\n",
        "    #     U = upred.numpy().reshape(N,N)\n",
        "    #     fig = plt.figure(figsize=(9,6))\n",
        "    #     ax = fig.add_subplot(111, projection='3d')\n",
        "    #     ax.plot_surface(T, X, Y, U, cmap='viridis', **kwargs)\n",
        "    #     ax.set_xlabel('$t$')\n",
        "    #     ax.set_ylabel('$x$')\n",
        "    #     ax.set_zlabel('$u_\\\\theta(t,x)$')\n",
        "    #     ax.view_init(35,35)\n",
        "    #     return ax\n",
        "\n",
        "    def plot_loss_history(self, ax=None):\n",
        "        if not ax:\n",
        "            fig = plt.figure(figsize=(7,5))\n",
        "            ax = fig.add_subplot(111)\n",
        "        ax.semilogy(range(len(self.hist)), self.hist,'k-')\n",
        "        ax.set_xlabel('$n_{epoch}$')\n",
        "        ax.set_ylabel('$\\\\phi^{n_{epoch}}$')\n",
        "        return ax\n",
        "\n",
        "    def plot_loss_and_param(self, axs=None):\n",
        "        if axs:\n",
        "            ax1, ax2 = axs\n",
        "            self.plot_loss_history(ax1)\n",
        "        else:\n",
        "            ax1 = self.plot_loss_history()\n",
        "            ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "        # color = 'tab:blue'\n",
        "        ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "        ax2.plot(range(len(self.hist)), self.model.lambd1_list,'-',color='tab:blue')\n",
        "        ax2.plot(range(len(self.hist)), self.model.lambd2_list,'-',color='tab:red')\n",
        "        ax2.set_ylabel('$\\\\lambda^{n_{epoch}}$', color='tab:blue')\n",
        "        return (ax1,ax2)"
      ],
      "metadata": {
        "id": "UBZF3jyotPAs"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize model\n",
        "model = PINNIdentificationNet(lb, ub, num_hidden_layers=2)\n",
        "model.build(input_shape=(None,3))\n",
        "\n",
        "# initialize PINN solver\n",
        "solver = PINNSolver(model, X)"
      ],
      "metadata": {
        "id": "BIqJTdChtQJc"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000],[1e-2,1e-3,5e-4])\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# start timer\n",
        "t0 = time()\n",
        "\n",
        "solver.solve_with_TFoptimizer(optim, X, data, N=6001)\n",
        "\n",
        "# Print computation time\n",
        "print('\\nComputation time: {} seconds'.format(time()-t0))"
      ],
      "metadata": {
        "id": "ppvRGri9tW4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca40f6ce-d89f-4eb0-fe67-271f254a5815"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It 00000: loss = 3.69023237e-01 lambda1 = 9.92622435e-01 lambda2 = 9.90040660e-01\n",
            "It 00100: loss = 2.33507923e-01 lambda1 = 7.59121180e-01 lambda2 = 1.80857673e-01\n",
            "It 00200: loss = 2.27127220e-01 lambda1 = 4.55356054e-02 lambda2 = 1.17634749e-03\n",
            "It 00300: loss = 2.25792526e-01 lambda1 = 4.65964377e-02 lambda2 = 8.74554971e-04\n",
            "It 00400: loss = 2.25481146e-01 lambda1 = 3.47143076e-02 lambda2 = 3.08643386e-04\n",
            "It 00500: loss = 2.25350592e-01 lambda1 = 2.86654308e-02 lambda2 = 3.40855913e-05\n",
            "It 00600: loss = 2.25199204e-01 lambda1 = 2.40895413e-02 lambda2 = 8.16821412e-05\n",
            "It 00700: loss = 2.25142129e-01 lambda1 = 2.06799470e-02 lambda2 = 1.51478162e-05\n",
            "It 00800: loss = 2.25103079e-01 lambda1 = 1.85826253e-02 lambda2 = 2.02592622e-04\n",
            "It 00900: loss = 2.25071027e-01 lambda1 = 1.74092501e-02 lambda2 = 2.54558545e-04\n",
            "It 01000: loss = 2.25033392e-01 lambda1 = 1.67424046e-02 lambda2 = 3.39110935e-04\n",
            "It 01100: loss = 2.25028574e-01 lambda1 = 1.68187730e-02 lambda2 = 2.78682914e-04\n",
            "It 01200: loss = 2.25023778e-01 lambda1 = 1.68427229e-02 lambda2 = 2.78689375e-04\n",
            "It 01300: loss = 2.25018650e-01 lambda1 = 1.68866832e-02 lambda2 = 2.77725019e-04\n",
            "It 01400: loss = 2.25013232e-01 lambda1 = 1.69505700e-02 lambda2 = 2.76097184e-04\n",
            "It 01500: loss = 2.25007575e-01 lambda1 = 1.70317143e-02 lambda2 = 2.73864454e-04\n",
            "It 01600: loss = 2.25001737e-01 lambda1 = 1.71240289e-02 lambda2 = 2.71143508e-04\n",
            "It 01700: loss = 2.24995776e-01 lambda1 = 1.72178727e-02 lambda2 = 2.68105330e-04\n",
            "It 01800: loss = 2.24989729e-01 lambda1 = 1.73006188e-02 lambda2 = 2.64961651e-04\n",
            "It 01900: loss = 2.24983601e-01 lambda1 = 1.73570141e-02 lambda2 = 2.61951558e-04\n",
            "It 02000: loss = 2.24977343e-01 lambda1 = 1.73698552e-02 lambda2 = 2.59332330e-04\n",
            "It 02100: loss = 2.24970831e-01 lambda1 = 1.73199419e-02 lambda2 = 2.57388718e-04\n",
            "It 02200: loss = 2.24963849e-01 lambda1 = 1.71862189e-02 lambda2 = 2.56507861e-04\n",
            "It 02300: loss = 2.24956095e-01 lambda1 = 1.69480518e-02 lambda2 = 2.57352513e-04\n",
            "It 02400: loss = 2.24947208e-01 lambda1 = 1.65917762e-02 lambda2 = 2.61037465e-04\n",
            "It 02500: loss = 2.24936846e-01 lambda1 = 1.61247458e-02 lambda2 = 2.68960284e-04\n",
            "It 02600: loss = 2.24924766e-01 lambda1 = 1.55926747e-02 lambda2 = 2.81740126e-04\n",
            "It 02700: loss = 2.24910913e-01 lambda1 = 1.50820548e-02 lambda2 = 2.97409075e-04\n",
            "It 02800: loss = 2.24895479e-01 lambda1 = 1.46933896e-02 lambda2 = 3.11165961e-04\n",
            "It 02900: loss = 2.24878882e-01 lambda1 = 1.44968592e-02 lambda2 = 3.18160106e-04\n",
            "It 03000: loss = 2.24861520e-01 lambda1 = 1.44967670e-02 lambda2 = 3.15964018e-04\n",
            "It 03100: loss = 2.24852431e-01 lambda1 = 1.45562543e-02 lambda2 = 3.11055221e-04\n",
            "It 03200: loss = 2.24843090e-01 lambda1 = 1.46420700e-02 lambda2 = 3.03445035e-04\n",
            "It 03300: loss = 2.24833417e-01 lambda1 = 1.47461928e-02 lambda2 = 2.92956975e-04\n",
            "It 03400: loss = 2.24823410e-01 lambda1 = 1.48608340e-02 lambda2 = 2.79496482e-04\n",
            "It 03500: loss = 2.24813042e-01 lambda1 = 1.49793588e-02 lambda2 = 2.63047637e-04\n",
            "It 03600: loss = 2.24802247e-01 lambda1 = 1.50973462e-02 lambda2 = 2.43647082e-04\n",
            "It 03700: loss = 2.24790898e-01 lambda1 = 1.52142188e-02 lambda2 = 2.21310984e-04\n",
            "It 03800: loss = 2.24778799e-01 lambda1 = 1.53343575e-02 lambda2 = 1.95911503e-04\n",
            "It 03900: loss = 2.24765679e-01 lambda1 = 1.54678477e-02 lambda2 = 1.67056409e-04\n",
            "It 04000: loss = 2.24751216e-01 lambda1 = 1.56298745e-02 lambda2 = 1.34137648e-04\n",
            "It 04100: loss = 2.24735117e-01 lambda1 = 1.58353429e-02 lambda2 = 9.67798551e-05\n",
            "It 04200: loss = 2.24717318e-01 lambda1 = 1.60824396e-02 lambda2 = 5.59354703e-05\n",
            "It 04300: loss = 2.24698292e-01 lambda1 = 1.63224358e-02 lambda2 = 1.52498342e-05\n",
            "It 04400: loss = 2.24679103e-01 lambda1 = 1.64395832e-02 lambda2 = -1.86910984e-05\n",
            "It 04500: loss = 2.24660791e-01 lambda1 = 1.62985865e-02 lambda2 = -3.95819952e-05\n",
            "It 04600: loss = 2.24643696e-01 lambda1 = 1.58335529e-02 lambda2 = -4.61435011e-05\n",
            "It 04700: loss = 2.24627586e-01 lambda1 = 1.50602255e-02 lambda2 = -4.21690856e-05\n",
            "It 04800: loss = 2.24612022e-01 lambda1 = 1.40234912e-02 lambda2 = -3.26863919e-05\n",
            "It 04900: loss = 2.24596392e-01 lambda1 = 1.27548007e-02 lambda2 = -2.13549029e-05\n",
            "It 05000: loss = 2.24579540e-01 lambda1 = 1.12390220e-02 lambda2 = -1.01166652e-05\n",
            "It 05100: loss = 2.24558017e-01 lambda1 = 9.35488567e-03 lambda2 = 4.13783596e-07\n",
            "It 05200: loss = 2.24512426e-01 lambda1 = 6.74301898e-03 lambda2 = 1.17997579e-05\n",
            "It 05300: loss = 2.24289622e-01 lambda1 = 2.43130326e-03 lambda2 = 1.36025819e-05\n",
            "It 05400: loss = 2.23811619e-01 lambda1 = -3.38527164e-03 lambda2 = 1.45880604e-05\n",
            "It 05500: loss = 2.23627715e-01 lambda1 = -3.43740475e-03 lambda2 = 7.86123783e-05\n",
            "It 05600: loss = 2.23560740e-01 lambda1 = -1.54700247e-03 lambda2 = 9.43442574e-05\n",
            "It 05700: loss = 2.23512438e-01 lambda1 = -6.01792766e-04 lambda2 = 8.85962145e-05\n",
            "It 05800: loss = 2.23469339e-01 lambda1 = -4.01443802e-04 lambda2 = 8.24458039e-05\n",
            "It 05900: loss = 2.23429466e-01 lambda1 = -6.26688357e-04 lambda2 = 7.34318746e-05\n",
            "It 06000: loss = 2.23392447e-01 lambda1 = -1.03456294e-03 lambda2 = 6.43082312e-05\n",
            "\n",
            "Computation time: 434.9028522968292 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# solver.plot_solution();"
      ],
      "metadata": {
        "id": "2dRrbkuttZxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solver.plot_loss_history();"
      ],
      "metadata": {
        "id": "1RCO0H91tiNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solver.plot_loss_and_param();"
      ],
      "metadata": {
        "id": "-xPYl_iqth0J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}