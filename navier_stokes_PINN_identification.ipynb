{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgINLKzLY4nuP4NM9gN+P9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yajuna/tensorflow_pde/blob/master/navier_stokes_PINN_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This work is based on the methodology originally developed by [Raissi](https://github.com/maziarraissi/PINNs/blob/master/main/continuous_time_identification%20(Navier-Stokes)/NavierStokes.py), and the Tensorflow 2.0 modification by [Blechschmidt](https://github.com/janblechschmidt/PDEsByNNs) and [pierremtb ](https://github.com/pierremtb/PINNs-TF2.0).\n",
        "\n",
        "A Tensorflow 2.0 implementation of the inference problem for the Navier Stokes equation is [here](https://github.com/yajuna/tensorflow_pde/blob/master/navier_stokes_PINN_Solver.ipynb). This notebook is a Tensorflow 2.0 implementation of the Navier Stokes identification problem. We run this notebook with the original data provided by the author to test our implementation for the inference problem. This code is modified from Blechschmidt, as well as [the heat identification code](https://github.com/yajuna/tensorflow_pde/blob/master/Heat_equation_with_tensorflow.ipynb)\n",
        "\n",
        "``tf.experimental.numpy.experimental_enable_numpy_behavior()`` helps numpy commands such as ``.min()`` for tf eager tensors."
      ],
      "metadata": {
        "id": "VXtYqqellzJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import scipy.io\n",
        "\n",
        "from time import time\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "fuZGtDwTqSfJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data for Navier Stokes\n",
        "\n",
        "Clone Raissi's original data for Navier Stokes equation."
      ],
      "metadata": {
        "id": "rXjg964rQD_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/maziarraissi/PINNs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8exWF5rNqrPF",
        "outputId": "79cf8f52-5283-4396-dd37-0005be87dac6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PINNs' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define path for data and Python scripts from Raissi."
      ],
      "metadata": {
        "id": "2bJ8HB2rcOAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
        "repoPath = os.path.join(\".\", \"PINNs\")\n",
        "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
        "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
        "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
        "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")"
      ],
      "metadata": {
        "id": "WfPE290ZrG2-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define data type and set seed for reproducibility involving random choices."
      ],
      "metadata": {
        "id": "gByNK5d9caXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set data type\n",
        "DTYPE='float32'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "tf.random.set_seed(0)"
      ],
      "metadata": {
        "id": "LYUP9e9v5Rug"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define number of training points ``N_train``. Read all data, ``U_star`` (with $u$ and $v$), ``P_star``, ``t_star``, and ``X_star`` (with $x$ and $y$).\n",
        "\n",
        "Data reshaped and stored in ``x,y,t,u,v,p``.\n",
        "\n",
        "A random instance was chosen by ``idx``, then a set of training points are chosen. Testing data is picked from the original data defined above and not randomized.\n",
        "\n",
        "Training data is listed in ``X`` and ``data``, testsing data is listed in ``X_test`` and ``data_test``."
      ],
      "metadata": {
        "id": "hMRvtKWCcoeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_train = 5000\n",
        "\n",
        "path = os.path.join(dataPath, \"cylinder_nektar_wake.mat\")\n",
        "data = scipy.io.loadmat(path)\n",
        "\n",
        "# define training and testing data\n",
        "U_star = data['U_star'] # N x 2 x T\n",
        "P_star = data['p_star'] # N x T\n",
        "t_star = data['t'] # T x 1\n",
        "X_star = data['X_star'] # N x 2\n",
        "\n",
        "N = X_star.shape[0]\n",
        "T = t_star.shape[0]\n",
        "\n",
        "print('N and T', N, T)\n",
        "\n",
        "#Rearrange Data\n",
        "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "TT = np.tile(t_star, (1,N)).T # N x T\n",
        "\n",
        "UU = U_star[:,0,:] # N x T\n",
        "VV = U_star[:,1,:] # N x T\n",
        "PP = P_star # N x T\n",
        "\n",
        "x = XX.flatten()[:,None] # NT x 1\n",
        "y = YY.flatten()[:,None] # NT x 1\n",
        "t = TT.flatten()[:,None] # NT x 1\n",
        "\n",
        "u = UU.flatten()[:,None] # NT x 1\n",
        "v = VV.flatten()[:,None] # NT x 1\n",
        "p = PP.flatten()[:,None] # NT x 1\n",
        "\n",
        "######################################################################\n",
        "######################## Noiseles Data ###############################\n",
        "######################################################################\n",
        "# Training Data\n",
        "idx = np.random.choice(N*T, N_train, replace=False)\n",
        "x_train = x[idx,:]\n",
        "y_train = y[idx,:]\n",
        "t_train = t[idx,:]\n",
        "u_train = u[idx,:]\n",
        "v_train = v[idx,:]\n",
        "p_train = p[idx,:]\n",
        "\n",
        "# Test Data\n",
        "snap = np.array([100])\n",
        "x_star = X_star[:,0:1]\n",
        "y_star = X_star[:,1:2]\n",
        "t_star = TT[:,snap]\n",
        "\n",
        "u_star = U_star[:,0,snap]\n",
        "v_star = U_star[:,1,snap]\n",
        "p_star = P_star[:,snap]\n",
        "\n",
        "# make training and testing data\n",
        "X = tf.concat([x_train,y_train,t_train], axis = 1)\n",
        "data = tf.concat([u_train,v_train,p_train], axis = 1)\n",
        "\n",
        "# print boundary range\n",
        "print(x.min())\n",
        "print(x.max())\n",
        "print(y.min())\n",
        "print(y.max())\n",
        "print(t.min())\n",
        "print(t.max())\n",
        "lb = tf.constant([x.min(), y.min(), t.min()],dtype = DTYPE)\n",
        "ub = tf.constant([x.max(), y.max(), t.max()],dtype = DTYPE)\n",
        "\n",
        "X_test = tf.concat([x_star,y_star,t_star], axis = 1)\n",
        "data_test = tf.concat([u_star,v_star,p_star], axis = 1)\n",
        "\n",
        "print('size of x_trian and x_star', x_train.size, x_star.size)\n",
        "\n",
        "## visualize training data for comparison"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifq23Johr2J-",
        "outputId": "b72eefeb-9c7e-426c-dbf1-3aa96207f8f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N and T 5000 200\n",
            "1.0\n",
            "8.0\n",
            "-2.0\n",
            "2.0\n",
            "0.0\n",
            "19.900000000000002\n",
            "size of x_trian and x_star 5000 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print data to compare with OpenFoam generated data."
      ],
      "metadata": {
        "id": "HY7PGAEny763"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print information from Raissi training data\n",
        "\"\"\"\n",
        "Recall training data is defined from\n",
        "\n",
        "U_star = data['U_star'] # N x 2 x T\n",
        "P_star = data['p_star'] # N x T\n",
        "t_star = data['t'] # T x 1\n",
        "X_star = data['X_star'] # N x 2\n",
        "\n",
        "then rearranged to be\n",
        "#Rearrange Data\n",
        "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "TT = np.tile(t_star, (1,N)).T # N x T\n",
        "\n",
        "UU = U_star[:,0,:] # N x T\n",
        "VV = U_star[:,1,:] # N x T\n",
        "PP = P_star # N x T\n",
        "\n",
        "x = XX.flatten()[:,None] # NT x 1\n",
        "y = YY.flatten()[:,None] # NT x 1\n",
        "t = TT.flatten()[:,None] # NT x 1\n",
        "\n",
        "u = UU.flatten()[:,None] # NT x 1\n",
        "v = VV.flatten()[:,None] # NT x 1\n",
        "p = PP.flatten()[:,None] # NT x 1\n",
        "\"\"\"\n",
        "\n",
        "# print('max and min of UU', UU.max(), UU.min())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "8THi02pozFpp",
        "outputId": "144971e3-57fb-4523-a3a7-08a67d0962e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nRecall training data is defined from\\n\\nU_star = data['U_star'] # N x 2 x T\\nP_star = data['p_star'] # N x T\\nt_star = data['t'] # T x 1\\nX_star = data['X_star'] # N x 2\\n\\nthen rearranged to be\\n#Rearrange Data\\nXX = np.tile(X_star[:,0:1], (1,T)) # N x T\\nYY = np.tile(X_star[:,1:2], (1,T)) # N x T\\nTT = np.tile(t_star, (1,N)).T # N x T\\n\\nUU = U_star[:,0,:] # N x T\\nVV = U_star[:,1,:] # N x T\\nPP = P_star # N x T\\n\\nx = XX.flatten()[:,None] # NT x 1\\ny = YY.flatten()[:,None] # NT x 1\\nt = TT.flatten()[:,None] # NT x 1\\n\\nu = UU.flatten()[:,None] # NT x 1\\nv = VV.flatten()[:,None] # NT x 1\\np = PP.flatten()[:,None] # NT x 1\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ################### load data from google drive\n",
        "# \"\"\"\n",
        "# To load data from Google Drive, download data from\n",
        "# https://github.com/maziarraissi/PINNs/tree/master/main/Data\n",
        "# and save it in a folder called \"data\" in your Google drive\n",
        "# \"\"\"\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# data = scipy.io.loadmat('/content/drive/My Drive/data/cylinder_nektar_wake.mat')"
      ],
      "metadata": {
        "id": "vB6m2Fx1QIBN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define PINN_NeuralNet, define model architecture.\n",
        "\n",
        "Input is three dimensional, consisting of $x$, $y$, and $t$. The output is one dimensional, consisting of concatenated $u$, $v$, and $p$.\n",
        "\n",
        "The neural net has one scaling layer, several hidden dense layers, and one output layer."
      ],
      "metadata": {
        "id": "rWgcSz_FlfYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model architecture\n",
        "\n",
        "class PINN_NeuralNet(tf.keras.Model):\n",
        "  \"\"\"Basic architecture of the PINN model\n",
        "  input dimension = 3, for x, y, t\n",
        "  output dimension = 1 for u, v, and p concatenated\n",
        "  \"\"\"\n",
        "# change input from lb, and ub to data from Raissi\n",
        "  def __init__(self, lb, ub,\n",
        "               output_dim = 1,\n",
        "               num_hidden_layers = 8,\n",
        "               num_neurons_per_layer = 20,\n",
        "               activation = 'tanh',\n",
        "               kernel_initializer = 'glorot_normal',\n",
        "               **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    self.lb = lb\n",
        "    self.ub = ub\n",
        "\n",
        "    # X = tf.concat([x,y,t], axis=1)\n",
        "    # self.lb = X.min(0)\n",
        "    # self.ub = X.max(0)\n",
        "    # self.X = X\n",
        "    # self.x = x\n",
        "    # self.y = y\n",
        "    # self.t = t\n",
        "    # self.u = u\n",
        "    # self.v = v\n",
        "\n",
        "    # Define NN architecture\n",
        "    self.scale = tf.keras.layers.Lambda(\n",
        "            lambda x: 2.0*(x - self.lb)/(self.ub - self.lb) - 1.0)\n",
        "    self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "                             activation=tf.keras.activations.get(activation),\n",
        "                             kernel_initializer=kernel_initializer)\n",
        "                           for _ in range(self.num_hidden_layers)]\n",
        "    self.out = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "  def call(self, X):\n",
        "    \"\"\"\n",
        "    Forward-pass thru NN\n",
        "    \"\"\"\n",
        "    Z = self.scale(X)\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      Z = self.hidden[i](Z)\n",
        "    return self.out(Z)"
      ],
      "metadata": {
        "id": "zYzfUI9wtBRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define PINNIdentificationNet, inherites from PINN_NeuralNet class.\n",
        "\n",
        "This neural net includes the two parameters to identify, call them ``lambd_1`` and ``lambd_2``."
      ],
      "metadata": {
        "id": "EZ2klcVdx-T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class PINNIdentificationNet(PINN_NeuralNet):\n",
        "#     def __init__(self, *args, **kwargs):\n",
        "\n",
        "#         # Call init of base class\n",
        "#         super().__init__(*args,**kwargs)\n",
        "\n",
        "#         # Initialize variable for lambda\n",
        "\n",
        "#         self.lambd1 = tf.Variable(1.0, trainable=True, dtype=DTYPE)\n",
        "#         self.lambd1_list = []\n",
        "\n",
        "#         self.lambd2 = tf.Variable(1.0, trainable=True, dtype=DTYPE)\n",
        "#         self.lambd2_list = []"
      ],
      "metadata": {
        "id": "MHfKtKPptIAf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define PINNSolver class."
      ],
      "metadata": {
        "id": "sctjRp720ikd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PINNSolver():\n",
        "    def __init__(self, model, X_r):\n",
        "        self.model = model\n",
        "\n",
        "        # Store collocation points, separate t and x\n",
        "        self.x = X_r[:,0:1]\n",
        "        self.y = X_r[:,1:2]\n",
        "        self.t = X_r[:,2:3]\n",
        "\n",
        "        # Initialize history of losses and global iteration counter\n",
        "        self.hist = []\n",
        "        self.iter = 0\n",
        "\n",
        "# differentiate to compute residual of PDE, physics information\n",
        "    def get_r(self):\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Watch variables representing t and x during this GradientTape\n",
        "            tape.watch(self.x)\n",
        "            tape.watch(self.y)\n",
        "            tape.watch(self.t)\n",
        "\n",
        "            # compute current psi and p; output of model\n",
        "            pp = self.model(tf.stack([self.x[:,0], self.y[:,0],self.t[:,0]], axis=1))\n",
        "            psi = pp[:,0:1]\n",
        "            p = pp[:,1:2]\n",
        "\n",
        "            # Compute gradients\n",
        "            u = tape.gradient(psi, self.y)\n",
        "            v1 = tape.gradient(psi, self.x)\n",
        "            v = -1 * v1\n",
        "            u_x = tape.gradient(u, self.x)\n",
        "            u_y = tape.gradient(u, self.y)\n",
        "\n",
        "            v_x = tape.gradient(v, self.x)\n",
        "            v_y = tape.gradient(v, self.y)\n",
        "\n",
        "\n",
        "\n",
        "        p_x = tape.gradient(p, self.x)\n",
        "        p_y = tape.gradient(p, self.y)\n",
        "\n",
        "        u_t = tape.gradient(u, self.t)\n",
        "        u_xx = tape.gradient(u_x, self.x)\n",
        "        u_yy = tape.gradient(u_y, self.y)\n",
        "\n",
        "        v_t = tape.gradient(v, self.t)\n",
        "        v_xx = tape.gradient(v_x, self.x)\n",
        "        v_yy = tape.gradient(v_y, self.y)\n",
        "\n",
        "        del tape\n",
        "\n",
        "        return self.fun_r(u, u_t, u_x, u_y, u_xx, u_yy, v, v_t, v_x, v_y, v_xx, v_yy, p_x, p_y)\n",
        "\n",
        "# compute loss function- physics information + computational error\n",
        "    def loss_fn(self, X, data):\n",
        "\n",
        "        # Compute phi_r from model and X_r\n",
        "        r = self.get_r()\n",
        "        phi_r = tf.reduce_mean(tf.square(r))\n",
        "\n",
        "        # Initialize loss\n",
        "        loss = phi_r\n",
        "\n",
        "        # compute loss at input data- concatenated u, v, p minus computed u, v, p\n",
        "\n",
        "\n",
        "        self.x = X[:,0:1]\n",
        "        self.y = X[:,1:2]\n",
        "        self.t = X[:,2:3]\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Watch variables representing t and x during this GradientTape\n",
        "            tape.watch(self.x)\n",
        "            tape.watch(self.y)\n",
        "            pp_pred = self.model(X)\n",
        "            psi_pred = pp_pred[:,0:1]\n",
        "\n",
        "        u_pred = tape.gradient(psi_pred, self.y)\n",
        "        v_pred1 = tape.gradient(psi_pred, self.x)\n",
        "        v_pred = -1 * v_pred1\n",
        "\n",
        "        p_pred = pp_pred[:,1:2]\n",
        "        data_pred = tf.concat([u_pred,v_pred,p_pred], axis = 1)\n",
        "        loss += tf.reduce_mean(tf.square(data - data_pred))\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "# from the model, get model trainable variables and keep for gradient descent\n",
        "    def get_grad(self, X, data):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # This tape is for derivatives with\n",
        "            # respect to trainable variables\n",
        "            tape.watch(self.model.trainable_variables)\n",
        "            loss = self.loss_fn(X, data)\n",
        "\n",
        "        g = tape.gradient(loss, self.model.trainable_variables)\n",
        "        del tape\n",
        "\n",
        "        return loss, g\n",
        "\n",
        "# compute residual from the neural net; physics information\n",
        "    def fun_r(self, u, u_t, u_x, u_y, u_xx, u_yy, v, v_t, v_x, v_y, v_xx, v_yy, p_x, p_y):\n",
        "        \"\"\"Residual of the PDE\"\"\"\n",
        "        # f = u_t + self.model.lambd1 * (u * u_x + v * u_y) + p_x - self.model.lambd2 * (u_xx + u_yy)\n",
        "        # g = v_t + self.model.lambd1 * (u * v_x + v * v_y) + p_y - self.model.lambd2 * (v_xx + v_yy)\n",
        "        f = u_t + (u * u_x + v * u_y) + p_x - 0.01 * (u_xx + u_yy)\n",
        "        g = v_t + (u * v_x + v * v_y) + p_y - 0.01 * (v_xx + v_yy)\n",
        "        return tf.concat([f, g], axis=1)\n",
        "\n",
        "    def solve_with_TFoptimizer(self, optimizer, X, data, N=1001):\n",
        "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
        "\n",
        "        @tf.function\n",
        "        def train_step():\n",
        "            loss, grad_theta = self.get_grad(X, data)\n",
        "\n",
        "            # Perform gradient descent step\n",
        "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
        "            return loss\n",
        "\n",
        "        for i in range(N):\n",
        "\n",
        "            loss = train_step()\n",
        "\n",
        "            self.current_loss = loss.numpy()\n",
        "            self.callback()\n",
        "\n",
        "    def callback(self, xr=None):\n",
        "        # lambd1 = self.model.lambd1.numpy()\n",
        "        # self.model.lambd1_list.append(lambd1)\n",
        "\n",
        "        # lambd2 = self.model.lambd2.numpy()\n",
        "        # self.model.lambd2_list.append(lambd2)\n",
        "\n",
        "        if self.iter % 100 == 0:\n",
        "            print('It {:05d}: loss = {:10.8e}'.format(self.iter, self.current_loss))\n",
        "\n",
        "        self.hist.append(self.current_loss)\n",
        "        self.iter += 1\n",
        "\n",
        "\n",
        "    def plot_solution(self, **kwargs):\n",
        "        N = 411\n",
        "        tspace = np.linspace(self.model.lb[0], self.model.ub[0], N)\n",
        "        xspace = np.linspace(self.model.lb[1], self.model.ub[1], N)\n",
        "        yspace = np.linspace(self.model.lb[2], self.model.ub[2], N)\n",
        "        T, X, Y = np.meshgrid(tspace, xspace, yspace)\n",
        "        Xgrid = np.vstack([T.flatten(),X.flatten(),Y.flatten()]).T\n",
        "        upred = self.model(tf.cast(Xgrid,DTYPE))\n",
        "        U = upred.numpy().reshape(N,N)\n",
        "        fig = plt.figure(figsize=(9,6))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.plot_surface(T, X, Y, U, cmap='viridis', **kwargs)\n",
        "        ax.set_xlabel('$t$')\n",
        "        ax.set_ylabel('$x$')\n",
        "        ax.set_zlabel('$u_\\\\theta(t,x)$')\n",
        "        ax.view_init(35,35)\n",
        "        return ax\n",
        "\n",
        "    def plot_loss_history(self, ax=None):\n",
        "        if not ax:\n",
        "            fig = plt.figure(figsize=(7,5))\n",
        "            ax = fig.add_subplot(111)\n",
        "        ax.semilogy(range(len(self.hist)), self.hist,'k-')\n",
        "        ax.set_xlabel('$n_{epoch}$')\n",
        "        ax.set_ylabel('$\\\\phi^{n_{epoch}}$')\n",
        "        return ax\n",
        "\n",
        "    def plot_loss_and_param(self, axs=None):\n",
        "        if axs:\n",
        "            ax1, ax2 = axs\n",
        "            self.plot_loss_history(ax1)\n",
        "        else:\n",
        "            ax1 = self.plot_loss_history()\n",
        "            ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "        # # color = 'tab:blue'\n",
        "        # ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "        # ax2.plot(range(len(self.hist)), self.model.lambd1_list,'-',color='tab:blue')\n",
        "        # ax2.plot(range(len(self.hist)), self.model.lambd2_list,'-',color='tab:red')\n",
        "        # ax2.set_ylabel('$\\\\lambda^{n_{epoch}}$', color='tab:blue')\n",
        "        return (ax1,ax2)"
      ],
      "metadata": {
        "id": "UBZF3jyotPAs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize model\n",
        "model = PINN_NeuralNet(lb, ub)\n",
        "model.build(input_shape=(None,3))\n",
        "\n",
        "# initialize PINN solver\n",
        "solver = PINNSolver(model, X)"
      ],
      "metadata": {
        "id": "BIqJTdChtQJc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000],[1e-2,1e-3,5e-4])\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# start timer\n",
        "t0 = time()\n",
        "\n",
        "solver.solve_with_TFoptimizer(optim, X, data, N=6001)\n",
        "\n",
        "# Print computation time\n",
        "print('\\nComputation time: {} seconds'.format(time()-t0))"
      ],
      "metadata": {
        "id": "ppvRGri9tW4T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "099c5d5a-304d-49e1-be18-9939c34b5182"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"<ipython-input-10-0435ac39dad6>\", line 116, in train_step  *\n        loss, grad_theta = self.get_grad(X, data)\n    File \"<ipython-input-10-0435ac39dad6>\", line 95, in get_grad  *\n        loss = self.loss_fn(X, data)\n    File \"<ipython-input-10-0435ac39dad6>\", line 80, in loss_fn  *\n        v_pred = -1 * v_pred1\n\n    TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b1dad0d8f56c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_with_TFoptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Print computation time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-0435ac39dad6>\u001b[0m in \u001b[0;36msolve_with_TFoptimizer\u001b[0;34m(self, optimizer, X, data, N)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file1t1ts5r0.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_theta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_theta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_fileypeoxeup.py\u001b[0m in \u001b[0;36mtf__get_grad\u001b[0;34m(self, X, data)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mtape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filenplbo92u.py\u001b[0m in \u001b[0;36mtf__loss_fn\u001b[0;34m(self, X, data)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mu_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mv_pred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mv_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_pred1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mp_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpp_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mdata_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"<ipython-input-10-0435ac39dad6>\", line 116, in train_step  *\n        loss, grad_theta = self.get_grad(X, data)\n    File \"<ipython-input-10-0435ac39dad6>\", line 95, in get_grad  *\n        loss = self.loss_fn(X, data)\n    File \"<ipython-input-10-0435ac39dad6>\", line 80, in loss_fn  *\n        v_pred = -1 * v_pred1\n\n    TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# solver.plot_solution();"
      ],
      "metadata": {
        "id": "2dRrbkuttZxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solver.plot_loss_history();"
      ],
      "metadata": {
        "id": "1RCO0H91tiNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solver.plot_loss_and_param();"
      ],
      "metadata": {
        "id": "-xPYl_iqth0J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}