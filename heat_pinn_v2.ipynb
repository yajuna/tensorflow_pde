{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORA2t5PFkX94RC7T6zlFdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yajuna/tensorflow_pde/blob/master/heat_pinn_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oQ7JnmcTSU4O"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set data type and hyperparameters\n",
        "\n",
        "DTYPE = 'float32'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "\n",
        "pi = tf.constant(np.pi, dtype = DTYPE)\n",
        "\n",
        "# Set number of data points\n",
        "N_0 = 412 # number of points in space; same as N_0 for simplicity\n",
        "N_b = 412 # number of points in time; this is the amount of measurements in 24 hours\n",
        "N_r = 1000\n",
        "\n",
        "# Set boundary\n",
        "tmin = 0.\n",
        "tmax = 24.\n",
        "xmin = 0.001\n",
        "xmax = 0.135\n",
        "\n",
        "# surface source to volume source\n",
        "delta_r = (xmax - xmin) / N_b\n",
        "coeff = 1. / delta_r\n",
        "\n",
        "# heat parameters\n",
        "sigma = 5.76e-8\n",
        "albedo = 0.3\n",
        "\n",
        "# Lower bounds in time and space\n",
        "lb = tf.constant([tmin, xmin], dtype=DTYPE)\n",
        "# Upper bounds in time and space\n",
        "ub = tf.constant([tmax, xmax], dtype=DTYPE)\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# Draw uniform sample points for initial boundary data; need N_0 == N_b for concat\n",
        "t_0 = tf.ones((N_0,1), dtype=DTYPE)*lb[0]\n",
        "x_init = tf.linspace(lb[1], ub[1], N_0)\n",
        "x_0 = tf.reshape(x_init, (N_0,1))\n",
        "X_0 = tf.concat([t_0, x_0], axis=1)\n",
        "\n",
        "# Boundary data- left to be core temperature, and right the bark temperature\n",
        "t_boundary = tf.linspace(lb[0], ub[0], N_b)\n",
        "t_b = tf.reshape(t_boundary, (N_b,1))\n",
        "x_lb = tf.ones((N_b,1), dtype=DTYPE)*lb[1]\n",
        "x_ub = tf.ones((N_b,1), dtype=DTYPE)*ub[1]\n",
        "X_lb = tf.concat([t_b, x_lb], axis=1)\n",
        "X_ub = tf.concat([t_b, x_ub], axis=1)\n",
        "\n",
        "## Add extra training data at X_train (location at midTemp1)\n",
        "idx = int(N_0 / 3)\n",
        "x_train = tf.ones((N_0,1), dtype=DTYPE) * x_init[idx]\n",
        "X_train = tf.concat([t_b, x_train], axis=1)\n",
        "x_train1 = tf.ones((N_0,1), dtype=DTYPE) * x_init[2 * idx]\n",
        "X_train1 = tf.concat([t_b, x_train1], axis=1)\n",
        "\n",
        "\n",
        "# Draw uniformly sampled collocation points\n",
        "t_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype=DTYPE)\n",
        "x_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype=DTYPE)\n",
        "X_r = tf.concat([t_r, x_r], axis=1)"
      ],
      "metadata": {
        "id": "YKH8KB-SSwQM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### ave tree temp and weather temp for future reference\n",
        "train_tree_temp_index = 416 - 2\n",
        "train_interp_temp_size = train_tree_temp_index\n",
        "\n",
        "train_weather_index = 145 - 2\n",
        "train_interp_weather_size = train_weather_index"
      ],
      "metadata": {
        "id": "pb1zjvAqS2d9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### measured tree temperature for initial and boundary conditions. Need fixing\n",
        "colnames_tree_temp = ['datetime', 's45_1', 'e9_1', 'n135_1','e45_2', 'n9_2', 'w135_2', 'n45_3', 'w9_3','s135_3', 'w_ext_35']\n",
        "url1 = \"https://raw.githubusercontent.com/yajuna/linearRegression/master/Tree_Temp_Values_AUG21_to_AUG28_2022.xlsx\"\n",
        "dataTemp = pandas.read_excel(url1,names=colnames_tree_temp)\n",
        "\n",
        "### training temperature data\n",
        "# core temp is west, at 13.5cm, at 2m high\n",
        "train_coreTemp = np.array(dataTemp.s135_3[2: train_tree_temp_index])#+ 273.15\n",
        "# West, at 9cm, at 3m high\n",
        "train_midTemp1 = np.array(dataTemp.w9_3[2: train_tree_temp_index])#+ 273.15\n",
        "# North, at 4.5cm, at 3m high\n",
        "train_midTemp2 = np.array(dataTemp.n45_3[2: train_tree_temp_index])#+ 273.15\n",
        "# bark temp is West, at bark, at 3.5m high\n",
        "train_barkTemp = np.array(dataTemp.w_ext_35[2: train_tree_temp_index])#+ 273.15\n",
        "\n",
        "train_initTemp = np.array([train_coreTemp[0], train_midTemp1[0], train_midTemp2[0], train_barkTemp[0]])\n",
        "train_init_temp = np.interp(np.linspace(0,xmax,N_0), np.linspace(0,xmax,train_initTemp.size),train_initTemp)\n"
      ],
      "metadata": {
        "id": "racafxWjS6Kw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u_0 = train_init_temp\n",
        "u_lb = train_coreTemp\n",
        "u_ub = train_barkTemp\n",
        "u_train = train_midTemp1\n",
        "u_train1 = train_midTemp2\n",
        "\n",
        "# collect initial and two boundary data in lists # add training data and points\n",
        "X_data = [X_0, X_lb, X_ub ]#, X_train]#, X_train1]\n",
        "u_data = [u_0, u_lb, u_ub ]#, u_train]#, u_train1]"
      ],
      "metadata": {
        "id": "S38Hq4K8TG89"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colnames_weather = ['datetime', 'wind_speed', 'wind_direction', 'humidity', 'air_temperature', 'air_pressure', 'solar_DNI']\n",
        "url2 = \"https://raw.githubusercontent.com/yajuna/linearRegression/master/Weather_Station_AUG21_to_AUG28_2022.xlsx\"\n",
        "dataWeather = pandas.read_excel(url2,names=colnames_weather)\n",
        "\n",
        "nu = 15.89e-6 # m^2/s kinematic viscosity of air\n",
        "Pr = 0.707\n",
        "ka = 26.3e-3 # W/mK conductivity of air\n",
        "coeff = 2 * xmax / nu\n",
        "\n",
        "#### training\n",
        "# wind speed from Oct 21\n",
        "train_windSpeed = np.array(dataWeather.wind_speed[:train_weather_index])\n",
        "# humidity from Oct 21\n",
        "train_humidity = np.array(dataWeather.humidity[:train_weather_index])\n",
        "# air temperature from Oct 21\n",
        "train_airTemp = np.array(dataWeather.air_temperature[:train_weather_index])#+ 273.15\n",
        "# air pressure from Oct 21\n",
        "train_airPressure = np.array(dataWeather.air_pressure[:train_weather_index])\n",
        "# solar radiation from Oct 21\n",
        "train_solar = np.array(dataWeather.solar_DNI[:train_weather_index])\n",
        "\n",
        "train_airTemp = np.interp(np.linspace(0,24,N_b), np.linspace(0,24,train_interp_weather_size),train_airTemp)\n",
        "train_windSpeed = np.interp(np.linspace(0,24,N_b), np.linspace(0,24,train_interp_weather_size),train_windSpeed)\n",
        "train_solar = np.interp(np.linspace(0,24,N_b), np.linspace(0,24,train_interp_weather_size),train_solar)\n",
        "\n",
        "\n",
        "\n",
        "Re_train = train_windSpeed * coeff\n",
        "C_train = []\n",
        "m_train = []\n",
        "for j in range(train_windSpeed.size):\n",
        "    temp = Re_train[j]\n",
        "    if temp < 0.4:\n",
        "        temp1 = [0, 0]\n",
        "    if 0.4<=temp<= 4:\n",
        "        temp1 = [0.989, 0.330]\n",
        "    if 4<temp<= 40:\n",
        "        temp1 = [0.911, 0.385]\n",
        "    if 40<temp<= 4e3:\n",
        "        temp1 = [0.683, 0.466]\n",
        "    if 4e3<temp<= 4e4:\n",
        "        temp1 = [0.193, 0.618]\n",
        "    if 4e4<temp<= 4e5:\n",
        "        temp1 = [0.027, 0.805]\n",
        "    C_train.append(temp1[0])\n",
        "    m_train.append(temp1[1])\n",
        "\n",
        "Nu_train = C_train * Re_train ** m_train * Pr ** (1/3)\n",
        "h_train = Nu_train * ka / (2 * xmax)\n",
        "\n",
        "convect_train = h_train * (u_ub - train_airTemp)\n",
        "sourceTerm_train = (1 - albedo) * train_solar + sigma * (train_airTemp**4 - u_ub**4) + convect_train\n"
      ],
      "metadata": {
        "id": "4PMU01UdTT-j"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lambd1 = 0.12/(1.38 * 510)\n",
        "# lambd2 = 1./(1.38 * 510)\n",
        "\n",
        "lambd1 = 0.12/(1380 * 510)\n",
        "lambd2 = 1./(1380 * 510)"
      ],
      "metadata": {
        "id": "fsezCFPvT4Tu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_model(num_hidden_layers=8, num_neurons_per_layer=20):\n",
        "    # Initialize a feedforward neural network\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Input is two-dimensional (time + one spatial dimension)\n",
        "    model.add(tf.keras.Input(2))\n",
        "\n",
        "    # Introduce a scaling layer to map input to [lb, ub]\n",
        "    scaling_layer = tf.keras.layers.Lambda(\n",
        "                lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)\n",
        "    model.add(scaling_layer)\n",
        "\n",
        "    # Append hidden layers\n",
        "    for _ in range(num_hidden_layers):\n",
        "        model.add(tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "            activation=tf.keras.activations.get('tanh'),\n",
        "            kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Output is one-dimensional\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "XX94Tq6ZT-MK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_r(model, X_r)-> tf.Tensor:\n",
        "\n",
        "    # A tf.GradientTape is used to compute derivatives in TensorFlow\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Split t and x to compute partial derivatives\n",
        "        t, x = X_r[:, 0:1], X_r[:,1:2]\n",
        "\n",
        "        # Variables t and x are watched during tape\n",
        "        # to compute derivatives u_t and u_x\n",
        "        tape.watch(t)\n",
        "        tape.watch(x)\n",
        "\n",
        "        # Determine residual\n",
        "        u = model(tf.stack([t[:,0], x[:,0]], axis=1))\n",
        "\n",
        "        # Compute gradient u_x within the GradientTape\n",
        "        # since we need second derivatives\n",
        "        u_x = tape.gradient(u, x)\n",
        "        u_t = tape.gradient(u, t)\n",
        "\n",
        "    u_xx = tape.gradient(u_x, x)\n",
        "\n",
        "    del tape\n",
        "\n",
        "    return u_t - 1./x * lambd1 * u_x - lambd1 * u_xx - lambd2 * sourceTerm_train"
      ],
      "metadata": {
        "id": "1q5nKOjbUCNy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = init_model(num_hidden_layers = 2)\n",
        "r = get_r(model, X_0)\n",
        "phi_r = tf.reduce_mean(tf.square(r))\n",
        "loss = phi_r"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAzHCzFEUFZf",
        "outputId": "4eb5b2df-11b7-44f7-9b09-06fb81945ae4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(model, X_r, X_data, u_data)-> tf.Tensor:\n",
        "\n",
        "    # Compute phi^r-- physics informed loss\n",
        "    r = get_r(model, X_r)\n",
        "    phi_r = tf.reduce_mean(tf.square(r))\n",
        "\n",
        "    # Initialize loss\n",
        "    loss = phi_r\n",
        "\n",
        "    # Add initial error loss; add boundary loss\n",
        "\n",
        "    for i in range(len(X_data)):\n",
        "        u_pred = model(X_data[i])\n",
        "        loss += tf.reduce_mean(tf.square(u_data[i] - u_pred))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    ## define customized loss-- max(abs(u_data and u_pred))\n",
        "\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "kvOCReqdUIPD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = compute_loss(model, X_r, X_data, u_data)"
      ],
      "metadata": {
        "id": "F7YEEFRUULOY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grad(model, X_r, X_data, u_data):\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # This tape is for derivatives with\n",
        "        # respect to trainable variables\n",
        "        tape.watch(model.trainable_variables)\n",
        "        loss = compute_loss(model, X_r, X_data, u_data)\n",
        "\n",
        "    g = tape.gradient(loss, model.trainable_variables)\n",
        "    del tape\n",
        "\n",
        "    return loss, g"
      ],
      "metadata": {
        "id": "nttNR9jsUNu-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model aka u_\\theta\n",
        "model = init_model(num_hidden_layers = 2)\n",
        "# print(model.layers[0].input_shape)\n",
        "\n",
        "# We choose a piecewise decay of the learning rate, i.e., the\n",
        "# step size in the gradient descent type algorithm\n",
        "# the first 1000 steps use a learning rate of 0.01\n",
        "# from 1000 - 3000: learning rate = 0.001\n",
        "# from 3000 onwards: learning rate = 0.0005\n",
        "\n",
        "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000],[1e-2,1e-3,5e-4])\n",
        "\n",
        "# Choose the optimizer\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=lr)"
      ],
      "metadata": {
        "id": "6ZFsi3jYUQnr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define one training step as a TensorFlow function to increase speed of training\n",
        "@tf.function\n",
        "def train_step()-> tf.Tensor:\n",
        "    # Compute current loss and gradient w.r.t. parameters\n",
        "    loss, grad_theta = get_grad(model, X_r, X_data, u_data)\n",
        "\n",
        "    # Perform gradient descent step\n",
        "    optim.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Number of training epochs\n",
        "EPOCH = 50000\n",
        "hist = []\n",
        "\n",
        "# Start timer\n",
        "t0 = time()\n",
        "\n",
        "for i in range(EPOCH+1):\n",
        "\n",
        "    loss = train_step()\n",
        "\n",
        "    # Append current loss to hist\n",
        "    hist.append(loss.numpy())\n",
        "\n",
        "    # Output current loss after 50 iterates\n",
        "    if i%50 == 0:\n",
        "        print('It {:05d}: loss = {:10.8e}'.format(i,loss))\n",
        "\n",
        "# Print computation time\n",
        "print('\\nComputation time: {} seconds'.format(time()-t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbzwJoZsUVWY",
        "outputId": "1fe0b2ea-c9b7-4d79-ae98-00ddcd17d981"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It 00000: loss = 6.11875977e+02\n",
            "It 00050: loss = 1.22301682e+02\n",
            "It 00100: loss = 2.05125751e+01\n",
            "It 00150: loss = 3.39507246e+00\n",
            "It 00200: loss = 6.04180396e-01\n",
            "It 00250: loss = 4.87179458e-01\n",
            "It 00300: loss = 4.66374487e-01\n",
            "It 00350: loss = 4.57041502e-01\n",
            "It 00400: loss = 4.51837838e-01\n",
            "It 00450: loss = 4.48576480e-01\n",
            "It 00500: loss = 4.46370006e-01\n",
            "It 00550: loss = 4.44793791e-01\n",
            "It 00600: loss = 4.43620980e-01\n",
            "It 00650: loss = 4.42720383e-01\n",
            "It 00700: loss = 4.42011029e-01\n",
            "It 00750: loss = 4.41440523e-01\n",
            "It 00800: loss = 4.40973699e-01\n",
            "It 00850: loss = 4.40585971e-01\n",
            "It 00900: loss = 4.40259963e-01\n",
            "It 00950: loss = 4.39982653e-01\n",
            "It 01000: loss = 4.39744562e-01\n",
            "It 01050: loss = 4.39718634e-01\n",
            "It 01100: loss = 4.39696252e-01\n",
            "It 01150: loss = 4.39673513e-01\n",
            "It 01200: loss = 4.39650327e-01\n",
            "It 01250: loss = 4.39626783e-01\n",
            "It 01300: loss = 4.39602911e-01\n",
            "It 01350: loss = 4.39578682e-01\n",
            "It 01400: loss = 4.39554214e-01\n",
            "It 01450: loss = 4.39529449e-01\n",
            "It 01500: loss = 4.39504474e-01\n",
            "It 01550: loss = 4.39479142e-01\n",
            "It 01600: loss = 4.39453691e-01\n",
            "It 01650: loss = 4.39428121e-01\n",
            "It 01700: loss = 4.39402282e-01\n",
            "It 01750: loss = 4.39376295e-01\n",
            "It 01800: loss = 4.39350218e-01\n",
            "It 01850: loss = 4.39324021e-01\n",
            "It 01900: loss = 4.39297765e-01\n",
            "It 01950: loss = 4.39271420e-01\n",
            "It 02000: loss = 4.39245015e-01\n",
            "It 02050: loss = 4.39218611e-01\n",
            "It 02100: loss = 4.39192146e-01\n",
            "It 02150: loss = 4.39165682e-01\n",
            "It 02200: loss = 4.39139247e-01\n",
            "It 02250: loss = 4.39112872e-01\n",
            "It 02300: loss = 4.39086467e-01\n",
            "It 02350: loss = 4.39060152e-01\n",
            "It 02400: loss = 4.39033985e-01\n",
            "It 02450: loss = 4.39007878e-01\n",
            "It 02500: loss = 4.38981771e-01\n",
            "It 02550: loss = 4.38955903e-01\n",
            "It 02600: loss = 4.38930064e-01\n",
            "It 02650: loss = 4.38904405e-01\n",
            "It 02700: loss = 4.38878953e-01\n",
            "It 02750: loss = 4.38853621e-01\n",
            "It 02800: loss = 4.38828498e-01\n",
            "It 02850: loss = 4.38803494e-01\n",
            "It 02900: loss = 4.38778639e-01\n",
            "It 02950: loss = 4.38754082e-01\n",
            "It 03000: loss = 4.38729674e-01\n",
            "It 03050: loss = 4.38717276e-01\n",
            "It 03100: loss = 4.38704967e-01\n",
            "It 03150: loss = 4.38692629e-01\n",
            "It 03200: loss = 4.38680142e-01\n",
            "It 03250: loss = 4.38667595e-01\n",
            "It 03300: loss = 4.38654989e-01\n",
            "It 03350: loss = 4.38642234e-01\n",
            "It 03400: loss = 4.38629419e-01\n",
            "It 03450: loss = 4.38616544e-01\n",
            "It 03500: loss = 4.38603550e-01\n",
            "It 03550: loss = 4.38590556e-01\n",
            "It 03600: loss = 4.38577443e-01\n",
            "It 03650: loss = 4.38564360e-01\n",
            "It 03700: loss = 4.38551158e-01\n",
            "It 03750: loss = 4.38537925e-01\n",
            "It 03800: loss = 4.38524634e-01\n",
            "It 03850: loss = 4.38511282e-01\n",
            "It 03900: loss = 4.38497990e-01\n",
            "It 03950: loss = 4.38484609e-01\n",
            "It 04000: loss = 4.38471138e-01\n",
            "It 04050: loss = 4.38457757e-01\n",
            "It 04100: loss = 4.38444346e-01\n",
            "It 04150: loss = 4.38430876e-01\n",
            "It 04200: loss = 4.38417405e-01\n",
            "It 04250: loss = 4.38403964e-01\n",
            "It 04300: loss = 4.38390464e-01\n",
            "It 04350: loss = 4.38377023e-01\n",
            "It 04400: loss = 4.38363582e-01\n",
            "It 04450: loss = 4.38350260e-01\n",
            "It 04500: loss = 4.38336790e-01\n",
            "It 04550: loss = 4.38323408e-01\n",
            "It 04600: loss = 4.38310117e-01\n",
            "It 04650: loss = 4.38296765e-01\n",
            "It 04700: loss = 4.38283533e-01\n",
            "It 04750: loss = 4.38270330e-01\n",
            "It 04800: loss = 4.38257158e-01\n",
            "It 04850: loss = 4.38244075e-01\n",
            "It 04900: loss = 4.38230991e-01\n",
            "It 04950: loss = 4.38217998e-01\n",
            "It 05000: loss = 4.38205063e-01\n",
            "It 05050: loss = 4.38192189e-01\n",
            "It 05100: loss = 4.38179433e-01\n",
            "It 05150: loss = 4.38166708e-01\n",
            "It 05200: loss = 4.38154101e-01\n",
            "It 05250: loss = 4.38141495e-01\n",
            "It 05300: loss = 4.38129038e-01\n",
            "It 05350: loss = 4.38116699e-01\n",
            "It 05400: loss = 4.38104391e-01\n",
            "It 05450: loss = 4.38092232e-01\n",
            "It 05500: loss = 4.38080162e-01\n",
            "It 05550: loss = 4.38068151e-01\n",
            "It 05600: loss = 4.38056290e-01\n",
            "It 05650: loss = 4.38044429e-01\n",
            "It 05700: loss = 4.38032806e-01\n",
            "It 05750: loss = 4.38021213e-01\n",
            "It 05800: loss = 4.38009769e-01\n",
            "It 05850: loss = 4.37998474e-01\n",
            "It 05900: loss = 4.37987179e-01\n",
            "It 05950: loss = 4.37976122e-01\n",
            "It 06000: loss = 4.37965184e-01\n",
            "It 06050: loss = 4.37954336e-01\n",
            "It 06100: loss = 4.37943578e-01\n",
            "It 06150: loss = 4.37932998e-01\n",
            "It 06200: loss = 4.37922478e-01\n",
            "It 06250: loss = 4.37912136e-01\n",
            "It 06300: loss = 4.37901914e-01\n",
            "It 06350: loss = 4.37891841e-01\n",
            "It 06400: loss = 4.37881887e-01\n",
            "It 06450: loss = 4.37872052e-01\n",
            "It 06500: loss = 4.37862366e-01\n",
            "It 06550: loss = 4.37852770e-01\n",
            "It 06600: loss = 4.37843353e-01\n",
            "It 06650: loss = 4.37834054e-01\n",
            "It 06700: loss = 4.37824875e-01\n",
            "It 06750: loss = 4.37815815e-01\n",
            "It 06800: loss = 4.37806994e-01\n",
            "It 06850: loss = 4.37798142e-01\n",
            "It 06900: loss = 4.37789559e-01\n",
            "It 06950: loss = 4.37780976e-01\n",
            "It 07000: loss = 4.37772661e-01\n",
            "It 07050: loss = 4.37764376e-01\n",
            "It 07100: loss = 4.37756300e-01\n",
            "It 07150: loss = 4.37748313e-01\n",
            "It 07200: loss = 4.37740505e-01\n",
            "It 07250: loss = 4.37732756e-01\n",
            "It 07300: loss = 4.37725186e-01\n",
            "It 07350: loss = 4.37717676e-01\n",
            "It 07400: loss = 4.37710375e-01\n",
            "It 07450: loss = 4.37703192e-01\n",
            "It 07500: loss = 4.37696069e-01\n",
            "It 07550: loss = 4.37689096e-01\n",
            "It 07600: loss = 4.37682271e-01\n",
            "It 07650: loss = 4.37675595e-01\n",
            "It 07700: loss = 4.37669009e-01\n",
            "It 07750: loss = 4.37662572e-01\n",
            "It 07800: loss = 4.37656224e-01\n",
            "It 07850: loss = 4.37649935e-01\n",
            "It 07900: loss = 4.37643826e-01\n",
            "It 07950: loss = 4.37637866e-01\n",
            "It 08000: loss = 4.37631994e-01\n",
            "It 08050: loss = 4.37626213e-01\n",
            "It 08100: loss = 4.37620491e-01\n",
            "It 08150: loss = 4.37614977e-01\n",
            "It 08200: loss = 4.37609524e-01\n",
            "It 08250: loss = 4.37604189e-01\n",
            "It 08300: loss = 4.37598944e-01\n",
            "It 08350: loss = 4.37593818e-01\n",
            "It 08400: loss = 4.37588811e-01\n",
            "It 08450: loss = 4.37583864e-01\n",
            "It 08500: loss = 4.37579066e-01\n",
            "It 08550: loss = 4.37574387e-01\n",
            "It 08600: loss = 4.37569767e-01\n",
            "It 08650: loss = 4.37565207e-01\n",
            "It 08700: loss = 4.37560737e-01\n",
            "It 08750: loss = 4.37556416e-01\n",
            "It 08800: loss = 4.37552154e-01\n",
            "It 08850: loss = 4.37548041e-01\n",
            "It 08900: loss = 4.37543929e-01\n",
            "It 08950: loss = 4.37539935e-01\n",
            "It 09000: loss = 4.37536061e-01\n",
            "It 09050: loss = 4.37532216e-01\n",
            "It 09100: loss = 4.37528461e-01\n",
            "It 09150: loss = 4.37524855e-01\n",
            "It 09200: loss = 4.37521219e-01\n",
            "It 09250: loss = 4.37517762e-01\n",
            "It 09300: loss = 4.37514365e-01\n",
            "It 09350: loss = 4.37510997e-01\n",
            "It 09400: loss = 4.37507749e-01\n",
            "It 09450: loss = 4.37504530e-01\n",
            "It 09500: loss = 4.37501431e-01\n",
            "It 09550: loss = 4.37498331e-01\n",
            "It 09600: loss = 4.37495381e-01\n",
            "It 09650: loss = 4.37492460e-01\n",
            "It 09700: loss = 4.37489629e-01\n",
            "It 09750: loss = 4.37486857e-01\n",
            "It 09800: loss = 4.37484056e-01\n",
            "It 09850: loss = 4.37481403e-01\n",
            "It 09900: loss = 4.37478840e-01\n",
            "It 09950: loss = 4.37476277e-01\n",
            "It 10000: loss = 4.37473834e-01\n",
            "It 10050: loss = 4.37471360e-01\n",
            "It 10100: loss = 4.37469035e-01\n",
            "It 10150: loss = 4.37466681e-01\n",
            "It 10200: loss = 4.37464476e-01\n",
            "It 10250: loss = 4.37462270e-01\n",
            "It 10300: loss = 4.37460095e-01\n",
            "It 10350: loss = 4.37457919e-01\n",
            "It 10400: loss = 4.37455893e-01\n",
            "It 10450: loss = 4.37453926e-01\n",
            "It 10500: loss = 4.37451959e-01\n",
            "It 10550: loss = 4.37450022e-01\n",
            "It 10600: loss = 4.37448144e-01\n",
            "It 10650: loss = 4.37446326e-01\n",
            "It 10700: loss = 4.37444568e-01\n",
            "It 10750: loss = 4.37442809e-01\n",
            "It 10800: loss = 4.37441081e-01\n",
            "It 10850: loss = 4.37439412e-01\n",
            "It 10900: loss = 4.37437832e-01\n",
            "It 10950: loss = 4.37436253e-01\n",
            "It 11000: loss = 4.37434703e-01\n",
            "It 11050: loss = 4.37433183e-01\n",
            "It 11100: loss = 4.37431723e-01\n",
            "It 11150: loss = 4.37430322e-01\n",
            "It 11200: loss = 4.37428892e-01\n",
            "It 11250: loss = 4.37427491e-01\n",
            "It 11300: loss = 4.37426209e-01\n",
            "It 11350: loss = 4.37424898e-01\n",
            "It 11400: loss = 4.37423617e-01\n",
            "It 11450: loss = 4.37422395e-01\n",
            "It 11500: loss = 4.37421143e-01\n",
            "It 11550: loss = 4.37419981e-01\n",
            "It 11600: loss = 4.37418848e-01\n",
            "It 11650: loss = 4.37417686e-01\n",
            "It 11700: loss = 4.37416643e-01\n",
            "It 11750: loss = 4.37415540e-01\n",
            "It 11800: loss = 4.37414557e-01\n",
            "It 11850: loss = 4.37413484e-01\n",
            "It 11900: loss = 4.37412471e-01\n",
            "It 11950: loss = 4.37411547e-01\n",
            "It 12000: loss = 4.37410563e-01\n",
            "It 12050: loss = 4.37409639e-01\n",
            "It 12100: loss = 4.37408745e-01\n",
            "It 12150: loss = 4.37407851e-01\n",
            "It 12200: loss = 4.37407017e-01\n",
            "It 12250: loss = 4.37406152e-01\n",
            "It 12300: loss = 4.37405378e-01\n",
            "It 12350: loss = 4.37404633e-01\n",
            "It 12400: loss = 4.37403798e-01\n",
            "It 12450: loss = 4.37403053e-01\n",
            "It 12500: loss = 4.37402278e-01\n",
            "It 12550: loss = 4.37401593e-01\n",
            "It 12600: loss = 4.37400848e-01\n",
            "It 12650: loss = 4.37400192e-01\n",
            "It 12700: loss = 4.37399507e-01\n",
            "It 12750: loss = 4.37398821e-01\n",
            "It 12800: loss = 4.37398225e-01\n",
            "It 12850: loss = 4.37397599e-01\n",
            "It 12900: loss = 4.37396973e-01\n",
            "It 12950: loss = 4.37396407e-01\n",
            "It 13000: loss = 4.37395811e-01\n",
            "It 13050: loss = 4.37395275e-01\n",
            "It 13100: loss = 4.37394679e-01\n",
            "It 13150: loss = 4.37394202e-01\n",
            "It 13200: loss = 4.37393636e-01\n",
            "It 13250: loss = 4.37393159e-01\n",
            "It 13300: loss = 4.37392592e-01\n",
            "It 13350: loss = 4.37392145e-01\n",
            "It 13400: loss = 4.37391728e-01\n",
            "It 13450: loss = 4.37391192e-01\n",
            "It 13500: loss = 4.37390804e-01\n",
            "It 13550: loss = 4.37390327e-01\n",
            "It 13600: loss = 4.37389910e-01\n",
            "It 13650: loss = 4.37389523e-01\n",
            "It 13700: loss = 4.37389046e-01\n",
            "It 13750: loss = 4.37388659e-01\n",
            "It 13800: loss = 4.37388331e-01\n",
            "It 13850: loss = 4.37387884e-01\n",
            "It 13900: loss = 4.37387556e-01\n",
            "It 13950: loss = 4.37387139e-01\n",
            "It 14000: loss = 4.37386811e-01\n",
            "It 14050: loss = 4.37386513e-01\n",
            "It 14100: loss = 4.37386096e-01\n",
            "It 14150: loss = 4.37385827e-01\n",
            "It 14200: loss = 4.37385499e-01\n",
            "It 14250: loss = 4.37385142e-01\n",
            "It 14300: loss = 4.37384874e-01\n",
            "It 14350: loss = 4.37384605e-01\n",
            "It 14400: loss = 4.37384278e-01\n",
            "It 14450: loss = 4.37384009e-01\n",
            "It 14500: loss = 4.37383711e-01\n",
            "It 14550: loss = 4.37383443e-01\n",
            "It 14600: loss = 4.37383205e-01\n",
            "It 14650: loss = 4.37382996e-01\n",
            "It 14700: loss = 4.37382698e-01\n",
            "It 14750: loss = 4.37382460e-01\n",
            "It 14800: loss = 4.37382251e-01\n",
            "It 14850: loss = 4.37381983e-01\n",
            "It 14900: loss = 4.37381744e-01\n",
            "It 14950: loss = 4.37381566e-01\n",
            "It 15000: loss = 4.37381357e-01\n",
            "It 15050: loss = 4.37381119e-01\n",
            "It 15100: loss = 4.37380910e-01\n",
            "It 15150: loss = 4.37380731e-01\n",
            "It 15200: loss = 4.37380522e-01\n",
            "It 15250: loss = 4.37380344e-01\n",
            "It 15300: loss = 4.37380165e-01\n",
            "It 15350: loss = 4.37379986e-01\n",
            "It 15400: loss = 4.37379777e-01\n",
            "It 15450: loss = 4.37379569e-01\n",
            "It 15500: loss = 4.37379420e-01\n",
            "It 15550: loss = 4.37379301e-01\n",
            "It 15600: loss = 4.37379062e-01\n",
            "It 15650: loss = 4.37378913e-01\n",
            "It 15700: loss = 4.37378794e-01\n",
            "It 15750: loss = 4.37378675e-01\n",
            "It 15800: loss = 4.37378526e-01\n",
            "It 15850: loss = 4.37378407e-01\n",
            "It 15900: loss = 4.37378198e-01\n",
            "It 15950: loss = 4.37378109e-01\n",
            "It 16000: loss = 4.37377989e-01\n",
            "It 16050: loss = 4.37377870e-01\n",
            "It 16100: loss = 4.37377781e-01\n",
            "It 16150: loss = 4.37377572e-01\n",
            "It 16200: loss = 4.37377483e-01\n",
            "It 16250: loss = 4.37377363e-01\n",
            "It 16300: loss = 4.37377274e-01\n",
            "It 16350: loss = 4.37377155e-01\n",
            "It 16400: loss = 4.37377006e-01\n",
            "It 16450: loss = 4.37376916e-01\n",
            "It 16500: loss = 4.37376827e-01\n",
            "It 16550: loss = 4.37376738e-01\n",
            "It 16600: loss = 4.37376678e-01\n",
            "It 16650: loss = 4.37376559e-01\n",
            "It 16700: loss = 4.37376469e-01\n",
            "It 16750: loss = 4.37376350e-01\n",
            "It 16800: loss = 4.37376261e-01\n",
            "It 16850: loss = 4.37376201e-01\n",
            "It 16900: loss = 4.37376112e-01\n",
            "It 16950: loss = 4.37376052e-01\n",
            "It 17000: loss = 4.37375933e-01\n",
            "It 17050: loss = 4.37375844e-01\n",
            "It 17100: loss = 4.37375784e-01\n",
            "It 17150: loss = 4.37375724e-01\n",
            "It 17200: loss = 4.37375665e-01\n",
            "It 17250: loss = 4.37375605e-01\n",
            "It 17300: loss = 4.37375486e-01\n",
            "It 17350: loss = 4.37375426e-01\n",
            "It 17400: loss = 4.37375367e-01\n",
            "It 17450: loss = 4.37375307e-01\n",
            "It 17500: loss = 4.37375218e-01\n",
            "It 17550: loss = 4.37375188e-01\n",
            "It 17600: loss = 4.37375128e-01\n",
            "It 17650: loss = 4.37375098e-01\n",
            "It 17700: loss = 4.37374979e-01\n",
            "It 17750: loss = 4.37374949e-01\n",
            "It 17800: loss = 4.37374890e-01\n",
            "It 17850: loss = 4.37374860e-01\n",
            "It 17900: loss = 4.37374800e-01\n",
            "It 17950: loss = 4.37374771e-01\n",
            "It 18000: loss = 4.37374711e-01\n",
            "It 18050: loss = 4.37374651e-01\n",
            "It 18100: loss = 4.37374622e-01\n",
            "It 18150: loss = 4.37374592e-01\n",
            "It 18200: loss = 4.37374562e-01\n",
            "It 18250: loss = 4.37374502e-01\n",
            "It 18300: loss = 4.37374413e-01\n",
            "It 18350: loss = 4.37374383e-01\n",
            "It 18400: loss = 4.37374413e-01\n",
            "It 18450: loss = 4.37374324e-01\n",
            "It 18500: loss = 4.37374264e-01\n",
            "It 18550: loss = 4.37374234e-01\n",
            "It 18600: loss = 4.37374204e-01\n",
            "It 18650: loss = 4.37374175e-01\n",
            "It 18700: loss = 4.37374145e-01\n",
            "It 18750: loss = 4.37374115e-01\n",
            "It 18800: loss = 4.37374085e-01\n",
            "It 18850: loss = 4.37374085e-01\n",
            "It 18900: loss = 4.37374055e-01\n",
            "It 18950: loss = 4.37374026e-01\n",
            "It 19000: loss = 4.37374711e-01\n",
            "It 19050: loss = 4.37373966e-01\n",
            "It 19100: loss = 4.37373906e-01\n",
            "It 19150: loss = 4.37374026e-01\n",
            "It 19200: loss = 4.37373847e-01\n",
            "It 19250: loss = 4.37373817e-01\n",
            "It 19300: loss = 4.37374085e-01\n",
            "It 19350: loss = 4.37373787e-01\n",
            "It 19400: loss = 4.37373787e-01\n",
            "It 19450: loss = 4.37373728e-01\n",
            "It 19500: loss = 4.37373698e-01\n",
            "It 19550: loss = 4.37373787e-01\n",
            "It 19600: loss = 4.37373668e-01\n",
            "It 19650: loss = 4.37373668e-01\n",
            "It 19700: loss = 4.37373638e-01\n",
            "It 19750: loss = 4.37373668e-01\n",
            "It 19800: loss = 4.37373608e-01\n",
            "It 19850: loss = 4.37373608e-01\n",
            "It 19900: loss = 4.37373579e-01\n",
            "It 19950: loss = 4.37373668e-01\n",
            "It 20000: loss = 4.37373549e-01\n",
            "It 20050: loss = 4.37373489e-01\n",
            "It 20100: loss = 4.37373608e-01\n",
            "It 20150: loss = 4.37373519e-01\n",
            "It 20200: loss = 4.37373459e-01\n",
            "It 20250: loss = 4.37373489e-01\n",
            "It 20300: loss = 4.37373489e-01\n",
            "It 20350: loss = 4.37373400e-01\n",
            "It 20400: loss = 4.37373459e-01\n",
            "It 20450: loss = 4.37373400e-01\n",
            "It 20500: loss = 4.37373430e-01\n",
            "It 20550: loss = 4.37373340e-01\n",
            "It 20600: loss = 4.37373340e-01\n",
            "It 20650: loss = 4.37373340e-01\n",
            "It 20700: loss = 4.37373400e-01\n",
            "It 20750: loss = 4.37373310e-01\n",
            "It 20800: loss = 4.37373310e-01\n",
            "It 20850: loss = 4.37373459e-01\n",
            "It 20900: loss = 4.37373281e-01\n",
            "It 20950: loss = 4.37373281e-01\n",
            "It 21000: loss = 4.37373281e-01\n",
            "It 21050: loss = 4.37373251e-01\n",
            "It 21100: loss = 4.37373340e-01\n",
            "It 21150: loss = 4.37373251e-01\n",
            "It 21200: loss = 4.37373251e-01\n",
            "It 21250: loss = 4.37373489e-01\n",
            "It 21300: loss = 4.37373221e-01\n",
            "It 21350: loss = 4.37373221e-01\n",
            "It 21400: loss = 4.37373221e-01\n",
            "It 21450: loss = 4.37373251e-01\n",
            "It 21500: loss = 4.37373191e-01\n",
            "It 21550: loss = 4.37373191e-01\n",
            "It 21600: loss = 4.37373191e-01\n",
            "It 21650: loss = 4.37373191e-01\n",
            "It 21700: loss = 4.37373191e-01\n",
            "It 21750: loss = 4.37373191e-01\n",
            "It 21800: loss = 4.37374175e-01\n",
            "It 21850: loss = 4.37373161e-01\n",
            "It 21900: loss = 4.37373161e-01\n",
            "It 21950: loss = 4.37373161e-01\n",
            "It 22000: loss = 4.37373161e-01\n",
            "It 22050: loss = 4.37373161e-01\n",
            "It 22100: loss = 4.37374443e-01\n",
            "It 22150: loss = 4.37373161e-01\n",
            "It 22200: loss = 4.37373161e-01\n",
            "It 22250: loss = 4.37373132e-01\n",
            "It 22300: loss = 4.37374592e-01\n",
            "It 22350: loss = 4.37373132e-01\n",
            "It 22400: loss = 4.37373132e-01\n",
            "It 22450: loss = 4.37373132e-01\n",
            "It 22500: loss = 4.37373132e-01\n",
            "It 22550: loss = 4.37373072e-01\n",
            "It 22600: loss = 4.37373072e-01\n",
            "It 22650: loss = 4.37373132e-01\n",
            "It 22700: loss = 4.37373132e-01\n",
            "It 22750: loss = 4.37373132e-01\n",
            "It 22800: loss = 4.37373132e-01\n",
            "It 22850: loss = 4.37373102e-01\n",
            "It 22900: loss = 4.37373102e-01\n",
            "It 22950: loss = 4.37373102e-01\n",
            "It 23000: loss = 4.37373102e-01\n",
            "It 23050: loss = 4.37373102e-01\n",
            "It 23100: loss = 4.37373370e-01\n",
            "It 23150: loss = 4.37373102e-01\n",
            "It 23200: loss = 4.37373132e-01\n",
            "It 23250: loss = 4.37373102e-01\n",
            "It 23300: loss = 4.37373042e-01\n",
            "It 23350: loss = 4.37373042e-01\n",
            "It 23400: loss = 4.37373102e-01\n",
            "It 23450: loss = 4.37373102e-01\n",
            "It 23500: loss = 4.37373102e-01\n",
            "It 23550: loss = 4.37373042e-01\n",
            "It 23600: loss = 4.37373102e-01\n",
            "It 23650: loss = 4.37373102e-01\n",
            "It 23700: loss = 4.37373012e-01\n",
            "It 23750: loss = 4.37373072e-01\n",
            "It 23800: loss = 4.37373012e-01\n",
            "It 23850: loss = 4.37373072e-01\n",
            "It 23900: loss = 4.37373072e-01\n",
            "It 23950: loss = 4.37373072e-01\n",
            "It 24000: loss = 4.37374532e-01\n",
            "It 24050: loss = 4.37373072e-01\n",
            "It 24100: loss = 4.37373072e-01\n",
            "It 24150: loss = 4.37373072e-01\n",
            "It 24200: loss = 4.37373072e-01\n",
            "It 24250: loss = 4.37373012e-01\n",
            "It 24300: loss = 4.37374264e-01\n",
            "It 24350: loss = 4.37373012e-01\n",
            "It 24400: loss = 4.37373072e-01\n",
            "It 24450: loss = 4.37373012e-01\n",
            "It 24500: loss = 4.37373012e-01\n",
            "It 24550: loss = 4.37373072e-01\n",
            "It 24600: loss = 4.37373072e-01\n",
            "It 24650: loss = 4.37373012e-01\n",
            "It 24700: loss = 4.37373012e-01\n",
            "It 24750: loss = 4.37373072e-01\n",
            "It 24800: loss = 4.37373072e-01\n",
            "It 24850: loss = 4.37373191e-01\n",
            "It 24900: loss = 4.37373072e-01\n",
            "It 24950: loss = 4.37372983e-01\n",
            "It 25000: loss = 4.37372983e-01\n",
            "It 25050: loss = 4.37373221e-01\n",
            "It 25100: loss = 4.37372983e-01\n",
            "It 25150: loss = 4.37373042e-01\n",
            "It 25200: loss = 4.37373489e-01\n",
            "It 25250: loss = 4.37372983e-01\n",
            "It 25300: loss = 4.37372983e-01\n",
            "It 25350: loss = 4.37374234e-01\n",
            "It 25400: loss = 4.37372983e-01\n",
            "It 25450: loss = 4.37372983e-01\n",
            "It 25500: loss = 4.37373132e-01\n",
            "It 25550: loss = 4.37372983e-01\n",
            "It 25600: loss = 4.37373310e-01\n",
            "It 25650: loss = 4.37373042e-01\n",
            "It 25700: loss = 4.37373042e-01\n",
            "It 25750: loss = 4.37372983e-01\n",
            "It 25800: loss = 4.37372983e-01\n",
            "It 25850: loss = 4.37372983e-01\n",
            "It 25900: loss = 4.37373042e-01\n",
            "It 25950: loss = 4.37372983e-01\n",
            "It 26000: loss = 4.37372983e-01\n",
            "It 26050: loss = 4.37373042e-01\n",
            "It 26100: loss = 4.37373042e-01\n",
            "It 26150: loss = 4.37372983e-01\n",
            "It 26200: loss = 4.37372983e-01\n",
            "It 26250: loss = 4.37373310e-01\n",
            "It 26300: loss = 4.37373042e-01\n",
            "It 26350: loss = 4.37373042e-01\n",
            "It 26400: loss = 4.37372983e-01\n",
            "It 26450: loss = 4.37372983e-01\n",
            "It 26500: loss = 4.37372983e-01\n",
            "It 26550: loss = 4.37374145e-01\n",
            "It 26600: loss = 4.37373042e-01\n",
            "It 26650: loss = 4.37372983e-01\n",
            "It 26700: loss = 4.37374502e-01\n",
            "It 26750: loss = 4.37372983e-01\n",
            "It 26800: loss = 4.37372983e-01\n",
            "It 26850: loss = 4.37373042e-01\n",
            "It 26900: loss = 4.37372983e-01\n",
            "It 26950: loss = 4.37372983e-01\n",
            "It 27000: loss = 4.37374324e-01\n",
            "It 27050: loss = 4.37372953e-01\n",
            "It 27100: loss = 4.37373012e-01\n",
            "It 27150: loss = 4.37372953e-01\n",
            "It 27200: loss = 4.37373012e-01\n",
            "It 27250: loss = 4.37372953e-01\n",
            "It 27300: loss = 4.37372953e-01\n",
            "It 27350: loss = 4.37372953e-01\n",
            "It 27400: loss = 4.37373102e-01\n",
            "It 27450: loss = 4.37373012e-01\n",
            "It 27500: loss = 4.37372953e-01\n",
            "It 27550: loss = 4.37372953e-01\n",
            "It 27600: loss = 4.37373906e-01\n",
            "It 27650: loss = 4.37372953e-01\n",
            "It 27700: loss = 4.37373012e-01\n",
            "It 27750: loss = 4.37372953e-01\n",
            "It 27800: loss = 4.37373012e-01\n",
            "It 27850: loss = 4.37373012e-01\n",
            "It 27900: loss = 4.37373281e-01\n",
            "It 27950: loss = 4.37372953e-01\n",
            "It 28000: loss = 4.37372953e-01\n",
            "It 28050: loss = 4.37373102e-01\n",
            "It 28100: loss = 4.37373012e-01\n",
            "It 28150: loss = 4.37372953e-01\n",
            "It 28200: loss = 4.37373012e-01\n",
            "It 28250: loss = 4.37373102e-01\n",
            "It 28300: loss = 4.37373012e-01\n",
            "It 28350: loss = 4.37372953e-01\n",
            "It 28400: loss = 4.37372953e-01\n",
            "It 28450: loss = 4.37373728e-01\n",
            "It 28500: loss = 4.37372953e-01\n",
            "It 28550: loss = 4.37372893e-01\n",
            "It 28600: loss = 4.37374115e-01\n",
            "It 28650: loss = 4.37373012e-01\n",
            "It 28700: loss = 4.37372953e-01\n",
            "It 28750: loss = 4.37372953e-01\n",
            "It 28800: loss = 4.37372893e-01\n",
            "It 28850: loss = 4.37373102e-01\n",
            "It 28900: loss = 4.37372953e-01\n",
            "It 28950: loss = 4.37373012e-01\n",
            "It 29000: loss = 4.37373012e-01\n",
            "It 29050: loss = 4.37373012e-01\n",
            "It 29100: loss = 4.37373012e-01\n",
            "It 29150: loss = 4.37372953e-01\n",
            "It 29200: loss = 4.37372953e-01\n",
            "It 29250: loss = 4.37372953e-01\n",
            "It 29300: loss = 4.37372953e-01\n",
            "It 29350: loss = 4.37372953e-01\n",
            "It 29400: loss = 4.37373012e-01\n",
            "It 29450: loss = 4.37373012e-01\n",
            "It 29500: loss = 4.37372953e-01\n",
            "It 29550: loss = 4.37372953e-01\n",
            "It 29600: loss = 4.37373012e-01\n",
            "It 29650: loss = 4.37373012e-01\n",
            "It 29700: loss = 4.37373012e-01\n",
            "It 29750: loss = 4.37373012e-01\n",
            "It 29800: loss = 4.37372953e-01\n",
            "It 29850: loss = 4.37373132e-01\n",
            "It 29900: loss = 4.37372953e-01\n",
            "It 29950: loss = 4.37372953e-01\n",
            "It 30000: loss = 4.37372953e-01\n",
            "It 30050: loss = 4.37373549e-01\n",
            "It 30100: loss = 4.37373012e-01\n",
            "It 30150: loss = 4.37372953e-01\n",
            "It 30200: loss = 4.37372953e-01\n",
            "It 30250: loss = 4.37373012e-01\n",
            "It 30300: loss = 4.37372953e-01\n",
            "It 30350: loss = 4.37372953e-01\n",
            "It 30400: loss = 4.37373012e-01\n",
            "It 30450: loss = 4.37373012e-01\n",
            "It 30500: loss = 4.37372953e-01\n",
            "It 30550: loss = 4.37373012e-01\n",
            "It 30600: loss = 4.37372953e-01\n",
            "It 30650: loss = 4.37372953e-01\n",
            "It 30700: loss = 4.37372953e-01\n",
            "It 30750: loss = 4.37373012e-01\n",
            "It 30800: loss = 4.37373281e-01\n",
            "It 30850: loss = 4.37372953e-01\n",
            "It 30900: loss = 4.37372953e-01\n",
            "It 30950: loss = 4.37372953e-01\n",
            "It 31000: loss = 4.37373012e-01\n",
            "It 31050: loss = 4.37372953e-01\n",
            "It 31100: loss = 4.37373012e-01\n",
            "It 31150: loss = 4.37372953e-01\n",
            "It 31200: loss = 4.37373012e-01\n",
            "It 31250: loss = 4.37372953e-01\n",
            "It 31300: loss = 4.37372953e-01\n",
            "It 31350: loss = 4.37372953e-01\n",
            "It 31400: loss = 4.37373012e-01\n",
            "It 31450: loss = 4.37372953e-01\n",
            "It 31500: loss = 4.37373012e-01\n",
            "It 31550: loss = 4.37373012e-01\n",
            "It 31600: loss = 4.37372953e-01\n",
            "It 31650: loss = 4.37373102e-01\n",
            "It 31700: loss = 4.37372953e-01\n",
            "It 31750: loss = 4.37373012e-01\n",
            "It 31800: loss = 4.37373012e-01\n",
            "It 31850: loss = 4.37373012e-01\n",
            "It 31900: loss = 4.37373012e-01\n",
            "It 31950: loss = 4.37372953e-01\n",
            "It 32000: loss = 4.37373370e-01\n",
            "It 32050: loss = 4.37372893e-01\n",
            "It 32100: loss = 4.37372953e-01\n",
            "It 32150: loss = 4.37374115e-01\n",
            "It 32200: loss = 4.37373012e-01\n",
            "It 32250: loss = 4.37372953e-01\n",
            "It 32300: loss = 4.37372953e-01\n",
            "It 32350: loss = 4.37372953e-01\n",
            "It 32400: loss = 4.37372953e-01\n",
            "It 32450: loss = 4.37373012e-01\n",
            "It 32500: loss = 4.37373012e-01\n",
            "It 32550: loss = 4.37372953e-01\n",
            "It 32600: loss = 4.37372953e-01\n",
            "It 32650: loss = 4.37373012e-01\n",
            "It 32700: loss = 4.37373012e-01\n",
            "It 32750: loss = 4.37372953e-01\n",
            "It 32800: loss = 4.37373102e-01\n",
            "It 32850: loss = 4.37372893e-01\n",
            "It 32900: loss = 4.37372953e-01\n",
            "It 32950: loss = 4.37373489e-01\n",
            "It 33000: loss = 4.37372953e-01\n",
            "It 33050: loss = 4.37372953e-01\n",
            "It 33100: loss = 4.37373012e-01\n",
            "It 33150: loss = 4.37373012e-01\n",
            "It 33200: loss = 4.37372953e-01\n",
            "It 33250: loss = 4.37373042e-01\n",
            "It 33300: loss = 4.37373012e-01\n",
            "It 33350: loss = 4.37373012e-01\n",
            "It 33400: loss = 4.37373549e-01\n",
            "It 33450: loss = 4.37372953e-01\n",
            "It 33500: loss = 4.37374115e-01\n",
            "It 33550: loss = 4.37373012e-01\n",
            "It 33600: loss = 4.37372953e-01\n",
            "It 33650: loss = 4.37373012e-01\n",
            "It 33700: loss = 4.37372953e-01\n",
            "It 33750: loss = 4.37372953e-01\n",
            "It 33800: loss = 4.37372953e-01\n",
            "It 33850: loss = 4.37373012e-01\n",
            "It 33900: loss = 4.37374115e-01\n",
            "It 33950: loss = 4.37372953e-01\n",
            "It 34000: loss = 4.37372953e-01\n",
            "It 34050: loss = 4.37373102e-01\n",
            "It 34100: loss = 4.37372953e-01\n",
            "It 34150: loss = 4.37372953e-01\n",
            "It 34200: loss = 4.37372953e-01\n",
            "It 34250: loss = 4.37372953e-01\n",
            "It 34300: loss = 4.37373012e-01\n",
            "It 34350: loss = 4.37372953e-01\n",
            "It 34400: loss = 4.37372953e-01\n",
            "It 34450: loss = 4.37373012e-01\n",
            "It 34500: loss = 4.37373012e-01\n",
            "It 34550: loss = 4.37373370e-01\n",
            "It 34600: loss = 4.37372953e-01\n",
            "It 34650: loss = 4.37373012e-01\n",
            "It 34700: loss = 4.37372953e-01\n",
            "It 34750: loss = 4.37373728e-01\n",
            "It 34800: loss = 4.37373012e-01\n",
            "It 34850: loss = 4.37372953e-01\n",
            "It 34900: loss = 4.37372953e-01\n",
            "It 34950: loss = 4.37372953e-01\n",
            "It 35000: loss = 4.37372953e-01\n",
            "It 35050: loss = 4.37372953e-01\n",
            "It 35100: loss = 4.37374830e-01\n",
            "It 35150: loss = 4.37373012e-01\n",
            "It 35200: loss = 4.37372953e-01\n",
            "It 35250: loss = 4.37372953e-01\n",
            "It 35300: loss = 4.37373012e-01\n",
            "It 35350: loss = 4.37372953e-01\n",
            "It 35400: loss = 4.37373012e-01\n",
            "It 35450: loss = 4.37372953e-01\n",
            "It 35500: loss = 4.37372893e-01\n",
            "It 35550: loss = 4.37372953e-01\n",
            "It 35600: loss = 4.37373638e-01\n",
            "It 35650: loss = 4.37373012e-01\n",
            "It 35700: loss = 4.37372953e-01\n",
            "It 35750: loss = 4.37372953e-01\n",
            "It 35800: loss = 4.37373012e-01\n",
            "It 35850: loss = 4.37373489e-01\n",
            "It 35900: loss = 4.37372953e-01\n",
            "It 35950: loss = 4.37372953e-01\n",
            "It 36000: loss = 4.37372953e-01\n",
            "It 36050: loss = 4.37372953e-01\n",
            "It 36100: loss = 4.37373042e-01\n",
            "It 36150: loss = 4.37373012e-01\n",
            "It 36200: loss = 4.37372953e-01\n",
            "It 36250: loss = 4.37373012e-01\n",
            "It 36300: loss = 4.37372953e-01\n",
            "It 36350: loss = 4.37372953e-01\n",
            "It 36400: loss = 4.37372953e-01\n",
            "It 36450: loss = 4.37372953e-01\n",
            "It 36500: loss = 4.37373012e-01\n",
            "It 36550: loss = 4.37373012e-01\n",
            "It 36600: loss = 4.37373012e-01\n",
            "It 36650: loss = 4.37373012e-01\n",
            "It 36700: loss = 4.37373012e-01\n",
            "It 36750: loss = 4.37373012e-01\n",
            "It 36800: loss = 4.37372953e-01\n",
            "It 36850: loss = 4.37372983e-01\n",
            "It 36900: loss = 4.37373787e-01\n",
            "It 36950: loss = 4.37372983e-01\n",
            "It 37000: loss = 4.37372923e-01\n",
            "It 37050: loss = 4.37373340e-01\n",
            "It 37100: loss = 4.37372983e-01\n",
            "It 37150: loss = 4.37372983e-01\n",
            "It 37200: loss = 4.37372923e-01\n",
            "It 37250: loss = 4.37372923e-01\n",
            "It 37300: loss = 4.37372983e-01\n",
            "It 37350: loss = 4.37372923e-01\n",
            "It 37400: loss = 4.37372923e-01\n",
            "It 37450: loss = 4.37373012e-01\n",
            "It 37500: loss = 4.37372923e-01\n",
            "It 37550: loss = 4.37372923e-01\n",
            "It 37600: loss = 4.37372983e-01\n",
            "It 37650: loss = 4.37374860e-01\n",
            "It 37700: loss = 4.37372923e-01\n",
            "It 37750: loss = 4.37372923e-01\n",
            "It 37800: loss = 4.37372923e-01\n",
            "It 37850: loss = 4.37373251e-01\n",
            "It 37900: loss = 4.37372983e-01\n",
            "It 37950: loss = 4.37372923e-01\n",
            "It 38000: loss = 4.37373072e-01\n",
            "It 38050: loss = 4.37372983e-01\n",
            "It 38100: loss = 4.37372923e-01\n",
            "It 38150: loss = 4.37372863e-01\n",
            "It 38200: loss = 4.37372923e-01\n",
            "It 38250: loss = 4.37373072e-01\n",
            "It 38300: loss = 4.37372923e-01\n",
            "It 38350: loss = 4.37372923e-01\n",
            "It 38400: loss = 4.37372923e-01\n",
            "It 38450: loss = 4.37372923e-01\n",
            "It 38500: loss = 4.37372923e-01\n",
            "It 38550: loss = 4.37373698e-01\n",
            "It 38600: loss = 4.37372923e-01\n",
            "It 38650: loss = 4.37372923e-01\n",
            "It 38700: loss = 4.37372923e-01\n",
            "It 38750: loss = 4.37372923e-01\n",
            "It 38800: loss = 4.37372923e-01\n",
            "It 38850: loss = 4.37372983e-01\n",
            "It 38900: loss = 4.37372923e-01\n",
            "It 38950: loss = 4.37372923e-01\n",
            "It 39000: loss = 4.37373430e-01\n",
            "It 39050: loss = 4.37372923e-01\n",
            "It 39100: loss = 4.37372923e-01\n",
            "It 39150: loss = 4.37372923e-01\n",
            "It 39200: loss = 4.37373072e-01\n",
            "It 39250: loss = 4.37372923e-01\n",
            "It 39300: loss = 4.37372923e-01\n",
            "It 39350: loss = 4.37372923e-01\n",
            "It 39400: loss = 4.37373519e-01\n",
            "It 39450: loss = 4.37372923e-01\n",
            "It 39500: loss = 4.37372923e-01\n",
            "It 39550: loss = 4.37372983e-01\n",
            "It 39600: loss = 4.37372923e-01\n",
            "It 39650: loss = 4.37372923e-01\n",
            "It 39700: loss = 4.37372923e-01\n",
            "It 39750: loss = 4.37372983e-01\n",
            "It 39800: loss = 4.37372983e-01\n",
            "It 39850: loss = 4.37372983e-01\n",
            "It 39900: loss = 4.37372923e-01\n",
            "It 39950: loss = 4.37372923e-01\n",
            "It 40000: loss = 4.37372923e-01\n",
            "It 40050: loss = 4.37372983e-01\n",
            "It 40100: loss = 4.37372983e-01\n",
            "It 40150: loss = 4.37372923e-01\n",
            "It 40200: loss = 4.37372983e-01\n",
            "It 40250: loss = 4.37372983e-01\n",
            "It 40300: loss = 4.37373072e-01\n",
            "It 40350: loss = 4.37372923e-01\n",
            "It 40400: loss = 4.37372983e-01\n",
            "It 40450: loss = 4.37372983e-01\n",
            "It 40500: loss = 4.37372923e-01\n",
            "It 40550: loss = 4.37372923e-01\n",
            "It 40600: loss = 4.37373072e-01\n",
            "It 40650: loss = 4.37372983e-01\n",
            "It 40700: loss = 4.37372923e-01\n",
            "It 40750: loss = 4.37372923e-01\n",
            "It 40800: loss = 4.37372983e-01\n",
            "It 40850: loss = 4.37372923e-01\n",
            "It 40900: loss = 4.37372923e-01\n",
            "It 40950: loss = 4.37372923e-01\n",
            "It 41000: loss = 4.37373161e-01\n",
            "It 41050: loss = 4.37372983e-01\n",
            "It 41100: loss = 4.37372923e-01\n",
            "It 41150: loss = 4.37372923e-01\n",
            "It 41200: loss = 4.37372923e-01\n",
            "It 41250: loss = 4.37372983e-01\n",
            "It 41300: loss = 4.37372983e-01\n",
            "It 41350: loss = 4.37372983e-01\n",
            "It 41400: loss = 4.37372983e-01\n",
            "It 41450: loss = 4.37372923e-01\n",
            "It 41500: loss = 4.37372923e-01\n",
            "It 41550: loss = 4.37372923e-01\n",
            "It 41600: loss = 4.37372923e-01\n",
            "It 41650: loss = 4.37374979e-01\n",
            "It 41700: loss = 4.37372923e-01\n",
            "It 41750: loss = 4.37372923e-01\n",
            "It 41800: loss = 4.37372923e-01\n",
            "It 41850: loss = 4.37372923e-01\n",
            "It 41900: loss = 4.37372923e-01\n",
            "It 41950: loss = 4.37372923e-01\n",
            "It 42000: loss = 4.37372983e-01\n",
            "It 42050: loss = 4.37372923e-01\n",
            "It 42100: loss = 4.37372923e-01\n",
            "It 42150: loss = 4.37372923e-01\n",
            "It 42200: loss = 4.37372923e-01\n",
            "It 42250: loss = 4.37372923e-01\n",
            "It 42300: loss = 4.37372923e-01\n",
            "It 42350: loss = 4.37372923e-01\n",
            "It 42400: loss = 4.37372983e-01\n",
            "It 42450: loss = 4.37372983e-01\n",
            "It 42500: loss = 4.37373072e-01\n",
            "It 42550: loss = 4.37372923e-01\n",
            "It 42600: loss = 4.37373847e-01\n",
            "It 42650: loss = 4.37372983e-01\n",
            "It 42700: loss = 4.37372923e-01\n",
            "It 42750: loss = 4.37373012e-01\n",
            "It 42800: loss = 4.37372923e-01\n",
            "It 42850: loss = 4.37373072e-01\n",
            "It 42900: loss = 4.37372983e-01\n",
            "It 42950: loss = 4.37372923e-01\n",
            "It 43000: loss = 4.37372983e-01\n",
            "It 43050: loss = 4.37373340e-01\n",
            "It 43100: loss = 4.37372923e-01\n",
            "It 43150: loss = 4.37372923e-01\n",
            "It 43200: loss = 4.37372923e-01\n",
            "It 43250: loss = 4.37372923e-01\n",
            "It 43300: loss = 4.37372923e-01\n",
            "It 43350: loss = 4.37373340e-01\n",
            "It 43400: loss = 4.37372923e-01\n",
            "It 43450: loss = 4.37372923e-01\n",
            "It 43500: loss = 4.37372923e-01\n",
            "It 43550: loss = 4.37372923e-01\n",
            "It 43600: loss = 4.37373161e-01\n",
            "It 43650: loss = 4.37372923e-01\n",
            "It 43700: loss = 4.37372983e-01\n",
            "It 43750: loss = 4.37372923e-01\n",
            "It 43800: loss = 4.37372923e-01\n",
            "It 43850: loss = 4.37372923e-01\n",
            "It 43900: loss = 4.37372983e-01\n",
            "It 43950: loss = 4.37372983e-01\n",
            "It 44000: loss = 4.37372923e-01\n",
            "It 44050: loss = 4.37372983e-01\n",
            "It 44100: loss = 4.37373251e-01\n",
            "It 44150: loss = 4.37372983e-01\n",
            "It 44200: loss = 4.37372923e-01\n",
            "It 44250: loss = 4.37372923e-01\n",
            "It 44300: loss = 4.37372983e-01\n",
            "It 44350: loss = 4.37372983e-01\n",
            "It 44400: loss = 4.37372923e-01\n",
            "It 44450: loss = 4.37372923e-01\n",
            "It 44500: loss = 4.37372983e-01\n",
            "It 44550: loss = 4.37372923e-01\n",
            "It 44600: loss = 4.37372923e-01\n",
            "It 44650: loss = 4.37372983e-01\n",
            "It 44700: loss = 4.37372923e-01\n",
            "It 44750: loss = 4.37372923e-01\n",
            "It 44800: loss = 4.37372923e-01\n",
            "It 44850: loss = 4.37372923e-01\n",
            "It 44900: loss = 4.37374085e-01\n",
            "It 44950: loss = 4.37372923e-01\n",
            "It 45000: loss = 4.37372983e-01\n",
            "It 45050: loss = 4.37372923e-01\n",
            "It 45100: loss = 4.37372923e-01\n",
            "It 45150: loss = 4.37372923e-01\n",
            "It 45200: loss = 4.37372923e-01\n",
            "It 45250: loss = 4.37372923e-01\n",
            "It 45300: loss = 4.37372923e-01\n",
            "It 45350: loss = 4.37372983e-01\n",
            "It 45400: loss = 4.37372923e-01\n",
            "It 45450: loss = 4.37372923e-01\n",
            "It 45500: loss = 4.37372983e-01\n",
            "It 45550: loss = 4.37372923e-01\n",
            "It 45600: loss = 4.37372923e-01\n",
            "It 45650: loss = 4.37372923e-01\n",
            "It 45700: loss = 4.37372923e-01\n",
            "It 45750: loss = 4.37372923e-01\n",
            "It 45800: loss = 4.37372983e-01\n",
            "It 45850: loss = 4.37372923e-01\n",
            "It 45900: loss = 4.37372923e-01\n",
            "It 45950: loss = 4.37372983e-01\n",
            "It 46000: loss = 4.37372923e-01\n",
            "It 46050: loss = 4.37372923e-01\n",
            "It 46100: loss = 4.37372923e-01\n",
            "It 46150: loss = 4.37373012e-01\n",
            "It 46200: loss = 4.37372923e-01\n",
            "It 46250: loss = 4.37372923e-01\n",
            "It 46300: loss = 4.37373519e-01\n",
            "It 46350: loss = 4.37372923e-01\n",
            "It 46400: loss = 4.37372923e-01\n",
            "It 46450: loss = 4.37372983e-01\n",
            "It 46500: loss = 4.37372923e-01\n",
            "It 46550: loss = 4.37372983e-01\n",
            "It 46600: loss = 4.37372983e-01\n",
            "It 46650: loss = 4.37372923e-01\n",
            "It 46700: loss = 4.37372923e-01\n",
            "It 46750: loss = 4.37373459e-01\n",
            "It 46800: loss = 4.37372863e-01\n",
            "It 46850: loss = 4.37372863e-01\n",
            "It 46900: loss = 4.37372923e-01\n",
            "It 46950: loss = 4.37372923e-01\n",
            "It 47000: loss = 4.37373072e-01\n",
            "It 47050: loss = 4.37372923e-01\n",
            "It 47100: loss = 4.37372923e-01\n",
            "It 47150: loss = 4.37372983e-01\n",
            "It 47200: loss = 4.37373072e-01\n",
            "It 47250: loss = 4.37372923e-01\n",
            "It 47300: loss = 4.37372923e-01\n",
            "It 47350: loss = 4.37372923e-01\n",
            "It 47400: loss = 4.37372923e-01\n",
            "It 47450: loss = 4.37372923e-01\n",
            "It 47500: loss = 4.37372923e-01\n",
            "It 47550: loss = 4.37372923e-01\n",
            "It 47600: loss = 4.37372923e-01\n",
            "It 47650: loss = 4.37372923e-01\n",
            "It 47700: loss = 4.37372923e-01\n",
            "It 47750: loss = 4.37372983e-01\n",
            "It 47800: loss = 4.37372983e-01\n",
            "It 47850: loss = 4.37373340e-01\n",
            "It 47900: loss = 4.37372983e-01\n",
            "It 47950: loss = 4.37372923e-01\n",
            "It 48000: loss = 4.37372983e-01\n",
            "It 48050: loss = 4.37372923e-01\n",
            "It 48100: loss = 4.37372923e-01\n",
            "It 48150: loss = 4.37372983e-01\n",
            "It 48200: loss = 4.37372983e-01\n",
            "It 48250: loss = 4.37372983e-01\n",
            "It 48300: loss = 4.37372983e-01\n",
            "It 48350: loss = 4.37372863e-01\n",
            "It 48400: loss = 4.37379777e-01\n",
            "It 48450: loss = 4.37372923e-01\n",
            "It 48500: loss = 4.37372923e-01\n",
            "It 48550: loss = 4.37372923e-01\n",
            "It 48600: loss = 4.37372923e-01\n",
            "It 48650: loss = 4.37372923e-01\n",
            "It 48700: loss = 4.37372923e-01\n",
            "It 48750: loss = 4.37372983e-01\n",
            "It 48800: loss = 4.37372863e-01\n",
            "It 48850: loss = 4.37372923e-01\n",
            "It 48900: loss = 4.37372923e-01\n",
            "It 48950: loss = 4.37372983e-01\n",
            "It 49000: loss = 4.37373161e-01\n",
            "It 49050: loss = 4.37372923e-01\n",
            "It 49100: loss = 4.37372863e-01\n",
            "It 49150: loss = 4.37373072e-01\n",
            "It 49200: loss = 4.37372983e-01\n",
            "It 49250: loss = 4.37372923e-01\n",
            "It 49300: loss = 4.37372983e-01\n",
            "It 49350: loss = 4.37372983e-01\n",
            "It 49400: loss = 4.37372923e-01\n",
            "It 49450: loss = 4.37372863e-01\n",
            "It 49500: loss = 4.37372923e-01\n",
            "It 49550: loss = 4.37373519e-01\n",
            "It 49600: loss = 4.37372923e-01\n",
            "It 49650: loss = 4.37372983e-01\n",
            "It 49700: loss = 4.37372923e-01\n",
            "It 49750: loss = 4.37372863e-01\n",
            "It 49800: loss = 4.37372923e-01\n",
            "It 49850: loss = 4.37372983e-01\n",
            "It 49900: loss = 4.37372983e-01\n",
            "It 49950: loss = 4.37372983e-01\n",
            "It 50000: loss = 4.37372923e-01\n",
            "\n",
            "Computation time: 221.6964304447174 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(9,6))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.semilogy(range(len(hist)), hist,'k-')\n",
        "ax.set_xlabel('$n_{epoch}$')\n",
        "ax.set_ylabel('$\\\\phi_{n_{epoch}}$');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "sQ8rN87ZUarn",
        "outputId": "0c680f97-176a-4f22-b78c-d024be6c54b9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}